{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "count the missing values of each dommain features, set the threshold to 0.3 to filter out the features with too many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020.0\n",
      "1948.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# data path\n",
    "file_path = '../data/cumulative_2022_v3_9_domain.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "column_to_variable_dict = np.load('../data/column_to_variable_dict.npy', allow_pickle=True).item()\n",
    "variable_to_column_dict = np.load('../data/variable_to_column_dict.npy', allow_pickle=True).item()\n",
    "\n",
    "# check the \"Year\" column's max and min value\n",
    "print(data['Year'].max())\n",
    "print(data['Year'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year                                  0\n",
       "South                              1801\n",
       "region                             1801\n",
       "racial_composition_nbhood         59420\n",
       "racial_composition_gradeSchool    60327\n",
       "                                  ...  \n",
       "occupation                        28223\n",
       "occupation14                      51795\n",
       "occupation71                      51795\n",
       "home_ownership                    13672\n",
       "urbanism                          24972\n",
       "Length: 116, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis the missing value of each column in differernt period:\n",
    "# 1. overall missing value and percentage\n",
    "# 2. missing value and percentage in recent 10 years (\"Year\" >= 2012)\")\n",
    "# 3. missing value and percentage in recent 20 years (\"Year\" >= 2002)\")\n",
    "# 4. missing value and percentage in recent 30 years (\"Year\" >= 1992)\")\n",
    "# 5. missing value and percentage in recent 40 years (\"Year\" >= 1982)\")\n",
    "# 6. missing value and percentage in recent 50 years (\"Year\" >= 1972)\")\n",
    "# 7. missing value and percentage in recent 60 years (\"Year\" >= 1962)\")\n",
    "\n",
    "# save the result in csv file, the first column is the feature name\n",
    "\n",
    "\n",
    "def missing_value_analysis(data):\n",
    "    # get the number of missing value of each column\n",
    "    missing_value_num = data.isnull().sum()\n",
    "    # get the percentage of missing value of each column\n",
    "    missing_value_percentage = missing_value_num / len(data)\n",
    "\n",
    "    missing_value_percentage_10 = data[data['Year'] >= 2012].isnull().sum() / len(data[data['Year'] >= 2012])\n",
    "    missing_value_percentage_20 = data[data['Year'] >= 2002].isnull().sum() / len(data[data['Year'] >= 2002])\n",
    "    missing_value_percentage_30 = data[data['Year'] >= 1992].isnull().sum() / len(data[data['Year'] >= 1992])\n",
    "    missing_value_percentage_40 = data[data['Year'] >= 1982].isnull().sum() / len(data[data['Year'] >= 1982])\n",
    "    missing_value_percentage_50 = data[data['Year'] >= 1972].isnull().sum() / len(data[data['Year'] >= 1972])\n",
    "    missing_value_percentage_60 = data[data['Year'] >= 1962].isnull().sum() / len(data[data['Year'] >= 1962])\n",
    "    missing_value_percentage_70 = data[data['Year'] >= 1952].isnull().sum() / len(data[data['Year'] >= 1952])\n",
    "\n",
    "    # get the variable name of each column by using the column_to_variable_dict\n",
    "    # missing_value_num.index = column_to_variable_dict['variable']\n",
    "\n",
    "\n",
    "    # combine the result\n",
    "    missing_value = pd.concat([missing_value_num, missing_value_percentage,\n",
    "                               missing_value_percentage_10, missing_value_percentage_20,\n",
    "                               missing_value_percentage_30, missing_value_percentage_40,\n",
    "                               missing_value_percentage_50, missing_value_percentage_60,    missing_value_percentage_70], axis=1)\n",
    "    missing_value.columns = ['missing_value_num', 'missing_value_percentage',\n",
    "                                'missing_value_percentage_10(>=2012)', 'missing_value_percentage_20(>=2002)',\n",
    "                                'missing_value_percentage_30(>=1992)', 'missing_value_percentage_40(>=1982)',\n",
    "                                'missing_value_percentage_50(>=1972)', 'missing_value_percentage_60(>=1962)', 'missing_value_percentage_60(>=1952)']\n",
    "\n",
    "    # sort the result by missing value percentage\n",
    "    missing_value = missing_value.sort_values(by='missing_value_percentage', ascending=False)\n",
    "\n",
    "\n",
    "    return missing_value\n",
    "\n",
    "missing_value = missing_value_analysis(data)\n",
    "\n",
    "# save the result\n",
    "# massing_value.to_csv('../data/missing_value_analysis.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one column to indicate the variable name of each row,using the index of the missing_value as the key\n",
    "\n",
    "variable_name = [ column_to_variable_dict[var] for var in missing_value.index]\n",
    "missing_value['variable_name'] = variable_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features used:  72\n",
      "number of features not used:  44\n"
     ]
    }
   ],
   "source": [
    "# set the filter-out thresholds:\n",
    "# 1. missing_value_percentage_10(>=2012) < 0.3\n",
    "# 2. missing_value_percentage_20(>=2002) < 0.4\n",
    "# 3. missing_value_percentage_30(>=1992) < 0.5\n",
    "\n",
    "threshold_10 = 0.3\n",
    "threshold_20 = 0.4\n",
    "threshold_30 = 0.5\n",
    "\n",
    "\n",
    "# filter out the features\n",
    "missing_value_used = missing_value[(\n",
    "                missing_value['missing_value_percentage_10(>=2012)'] < threshold_10) & \n",
    "                                        (missing_value['missing_value_percentage_20(>=2002)'] < threshold_20) &\n",
    "                                        (missing_value['missing_value_percentage_30(>=1992)'] < threshold_30)]\n",
    "\n",
    "missing_value_not_used = missing_value[(\n",
    "                missing_value['missing_value_percentage_10(>=2012)'] >= threshold_10) | \n",
    "                                        (missing_value['missing_value_percentage_20(>=2002)'] >= threshold_20) |\n",
    "                                        (missing_value['missing_value_percentage_30(>=1992)'] >= threshold_30)]\n",
    "\n",
    "# count the number of features\n",
    "print('number of features used: ', len(missing_value_used))\n",
    "print('number of features not used: ', len(missing_value_not_used))\n",
    "\n",
    "\n",
    "\n",
    "# save the result\n",
    "# make folder namsed with threshold:\n",
    "\n",
    "folder_name = '../data/threshold_10_' + str(threshold_10) + '_threshold_20_' + str(threshold_20) + '_threshold_30_' + str(threshold_30)\n",
    "\n",
    "# make folder if not exist\n",
    "import os\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# missing_value_used.to_csv(folder_name + '/missing_value_analysis_used.csv')\n",
    "# missing_value_not_used.to_csv(folder_name + '/missing_value_analysis_not_used.csv')\n",
    "\n",
    "# save the used features names (row names) and the variable names\n",
    "used_features = missing_value_used.index.tolist()\n",
    "with open(folder_name + '/used_features.txt', 'w') as f:\n",
    "    for item in used_features:\n",
    "        f.write(\"%s (%s)\\n\" % (item, column_to_variable_dict[item]))\n",
    "\n",
    "# save the not used features names (row names)\n",
    "not_used_features = missing_value_not_used.index.tolist()\n",
    "with open(folder_name + '/not_used_features.txt', 'w') as f:\n",
    "    for item in not_used_features:\n",
    "        f.write(\"%s (%s)\\n\" % (item, column_to_variable_dict[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing value of the target variable: \n",
      "Voted                0.091551\n",
      "Registered_voted     0.218061\n",
      "Voted_party          0.536483\n",
      "Vote_Nonvote_Pres    0.377067\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# use the used features to filter out the data\n",
    "\n",
    "# set the target variable set and index variable set, these variables will not be used for training\n",
    "\n",
    "target_variable_list = ['Voted','Registered_voted','Voted_party','Vote_Nonvote_Pres']\n",
    "\n",
    "index_variable_list = ['Year', ]\n",
    "\n",
    "# check the missing ratio of the target variable\n",
    "print('missing value of the target variable: ')\n",
    "print(data[target_variable_list].isnull().sum() / len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples who not vote :  17790\n",
      "number of samples who vote :  44188\n",
      "number of samples who vote case DK :  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(61978, 116)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_variable = 'Voted'\n",
    "\n",
    "'''Voted  {0.0: '0. DK; NA; no Post IW; refused to say if voted;', 1.0: '1. No, did not vote', 2.0: '2. Yes, voted'}'''\n",
    "\n",
    "# filter out the samples with missing value of the target variable,drop the index\n",
    "data_new = data[data[target_variable].notnull()]\n",
    "# filter out the samples with target variable value = 0, count the number of samples whose target variable value = 0, 1 or 2\n",
    "print('number of samples who not vote : ', len(data_new[data_new[target_variable] == 1]))\n",
    "print('number of samples who vote : ', len(data_new[data_new[target_variable] == 2]))\n",
    "print('number of samples who vote case DK : ', len(data_new[data_new[target_variable] == 0]))\n",
    "\n",
    "data_new = data_new[data_new[target_variable] != 0]\n",
    "data_new = data_new.reset_index(drop=True)\n",
    "data_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Voted\n",
       "2.0    44188\n",
       "1.0    17790\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new['Voted'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of numerical features:  11\n",
      "number of categorical features:  56\n",
      "numerical features list: ['therm_Christians', 'therm_Mislims', 'therm_ChrFundament', 'therm_hispanics', 'therm_RepParty', 'therm_DemParty', 'therm_Whites', 'therm_liberals', 'therm_conservatives', 'therm_Blacks', 'Age']\n"
     ]
    }
   ],
   "source": [
    "# go through all used features, check the num of the categories of each feature: if the num of categories > 10, then this feature is a continuous/numerical feature, otherwise, this feature is a categorical feature-> need to do one-hot encoding\n",
    "\n",
    "numerical_feature_list = []\n",
    "categorical_feature_list = []\n",
    "\n",
    "for feature in used_features:\n",
    "\n",
    "    if feature not in target_variable_list and feature not in index_variable_list:\n",
    "\n",
    "        if len(data_new[feature].value_counts()) > 10:\n",
    "            numerical_feature_list.append(feature)\n",
    "        else:\n",
    "            categorical_feature_list.append(feature)\n",
    "\n",
    "print('number of numerical features: ', len(numerical_feature_list))\n",
    "\n",
    "print('number of categorical features: ', len(categorical_feature_list))\n",
    "\n",
    "print('numerical features list:',numerical_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61978, 68)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Voted\n",
       "2.0    44188\n",
       "1.0    17790\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# start from all-clear case:  further filter out the samples with missing value of the used features\n",
    "\n",
    "data_XY = data_new[numerical_feature_list + categorical_feature_list+[target_variable]]\n",
    "# data_XY = data_XY[data_XY.notnull().all(axis=1)]\n",
    "# data_XY = data_XY.reset_index(drop=True)\n",
    "print(data_XY.shape)\n",
    "\n",
    "X_continuous = data_XY[numerical_feature_list]\n",
    "X_categorical = data_XY[categorical_feature_list]\n",
    "Y_target = data_XY[target_variable]\n",
    "\n",
    "Y_target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Voted\n",
       "2.0    5087\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7153382811659675\n",
      "recall:  0.03750234389649353\n",
      "precision:  0.5555555555555556\n",
      "f1 score:  0.07026172492534692\n",
      "number of predicted samples for each class: \n",
      "2.0    18234\n",
      "1.0      360\n",
      "Name: count, dtype: int64\n",
      "model just predict the majority class:  0.01936108422071636\n"
     ]
    }
   ],
   "source": [
    "# only use the continuous features to do logistic regression by sklearn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# impute the missing value of the continuous features by using the mean value of the feature\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "X_continuous_imp = imp.fit_transform(X_continuous)\n",
    "\n",
    "X_continuous_transformed = StandardScaler().fit_transform(X_continuous_imp)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_continuous_transformed, Y_target, test_size=0.3, random_state=0)\n",
    "\n",
    "# use the default parameters\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "\n",
    "# get the accuracy, recall, precision, f1 score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "y_pred = logisticRegr.predict(X_test)\n",
    "\n",
    "print('accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('recall: ', recall_score(y_test, y_pred))\n",
    "print('precision: ', precision_score(y_test, y_pred))\n",
    "print('f1 score: ', f1_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "# count the number of predicted samples for each class\n",
    "print('number of predicted samples for each class: ')\n",
    "print(pd.Series(y_pred).value_counts())\n",
    "\n",
    "print('model just predict the majority class: ', pd.Series(y_pred).value_counts()[1] / len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8755512530923953\n",
      "recall:  0.7314832177011064\n",
      "precision:  0.8155969057077148\n",
      "f1 score:  0.7712534598655595\n",
      "number of predicted samples for each class: \n",
      "2.0    13811\n",
      "1.0     4783\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zenus/anaconda3/envs/torch_1.13_gpu/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# only use the categorical features to do logistic regression by sklearn, for the NaN value, set as a new category, then do one-hot encoding\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X_categorical_imp = X_categorical.fillna(-1)\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(X_categorical_imp)\n",
    "X_categorical_transformed = enc.transform(X_categorical_imp).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_categorical_transformed, Y_target, test_size=0.3, random_state=0)\n",
    "\n",
    "# use the default parameters\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "\n",
    "# get the accuracy, recall, precision, f1 score\n",
    "\n",
    "y_pred = logisticRegr.predict(X_test)\n",
    "\n",
    "print('accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('recall: ', recall_score(y_test, y_pred))\n",
    "print('precision: ', precision_score(y_test, y_pred))\n",
    "print('f1 score: ', f1_score(y_test, y_pred))\n",
    "\n",
    "# count the number of predicted samples for each class\n",
    "print('number of predicted samples for each class: ')\n",
    "print(pd.Series(y_pred).value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 features: \n",
      "('Feature_id 7', 'sex_orientation', 'Category 1', -1.0)\n",
      "('Feature_id 44', 'Interest_elections', 'Category 4', 3.0)\n",
      "('Feature_id 25', 'VCF9028', 'Category 4', 3.0)\n",
      "('Feature_id 28', 'Pre_election_inten_vote', 'Category 4', 3.0)\n",
      "('Feature_id 21', 'church_attendance', 'Category 1', -1.0)\n",
      "('Feature_id 18', 'volunteer', 'Category 1', -1.0)\n",
      "('Feature_id 28', 'Pre_election_inten_vote', 'Category 5', 4.0)\n",
      "('Feature_id 12', 'VCF9022', 'Category 3', 5.0)\n",
      "('Feature_id 12', 'VCF9022', 'Category 2', 1.0)\n",
      "('Feature_id 12', 'VCF9022', 'Category 1', -1.0)\n"
     ]
    }
   ],
   "source": [
    "# 建立映射\n",
    "feature_mapping = {}\n",
    "current_index = 0\n",
    "for feature_index, categories in enumerate(enc.categories_):\n",
    "    for category_index, category in enumerate(categories):\n",
    "        feature_mapping[current_index] = (f'Feature_id {feature_index}', \n",
    "         enc.feature_names_in_[feature_index],                             \n",
    "        f'Category {category_index + 1}', category)\n",
    "        current_index += 1\n",
    "\n",
    "# # 打印映射结果\n",
    "# print(\"Feature Mapping:\")\n",
    "# for k, v in feature_mapping.items():\n",
    "#     print(f\"Encoded feature {k}: Original {v}\")\n",
    "\n",
    "# identify the top 10 features that have the largest absolute value of the coefficient,\n",
    "\n",
    "top_10_index = np.argsort(np.abs(logisticRegr.coef_[0]))[-10:]\n",
    "\n",
    "# print the top 10 features\n",
    "print('top 10 features: ')\n",
    "for index in top_10_index:\n",
    "    print(feature_mapping[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8801226202000645\n",
      "recall:  0.7419838739921245\n",
      "precision:  0.8226611226611227\n",
      "f1 score:  0.7802425317953268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zenus/anaconda3/envs/torch_1.13_gpu/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# concatenate the continuous features and categorical features, then do logistic regression\n",
    "\n",
    "X_continuous_categorical = np.concatenate((X_continuous_transformed, X_categorical_transformed), axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_continuous_categorical, Y_target, test_size=0.3, random_state=0)\n",
    "\n",
    "# use the default parameters\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "\n",
    "# get the accuracy, recall, precision, f1 score\n",
    "\n",
    "y_pred = logisticRegr.predict(X_test)\n",
    "\n",
    "print('accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('recall: ', recall_score(y_test, y_pred))\n",
    "print('precision: ', precision_score(y_test, y_pred))\n",
    "print('f1 score: ', f1_score(y_test, y_pred))\n",
    "\n",
    "# check the top 10 features with the highest absolute value of the coefficient\n",
    "\n",
    "feature_importance = pd.DataFrame({'feature': numerical_feature_list + enc.get_feature_names().tolist(), 'importance': logisticRegr.coef_[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.36331930e-02,  3.57453106e-02, -6.44025252e-02, -2.32044616e-02,\n",
       "       -1.57759918e-02,  1.24256973e-01, -6.48274198e-02, -3.76747785e-02,\n",
       "        1.11533412e-02,  4.39356074e-02,  4.74777188e-01,  5.92446159e-01,\n",
       "        1.20615192e-01, -1.62057963e-01, -8.29689575e-02, -2.28963544e-01,\n",
       "        3.84357878e-01,  1.30349627e-01,  3.02776980e-01, -9.92210552e-02,\n",
       "       -2.12654556e-02, -4.78535804e-01,  8.39650357e-02, -1.31819897e-01,\n",
       "        8.12838989e-02,  2.11760017e-01,  7.01381181e-01,  1.14074207e-01,\n",
       "        8.49576313e-02,  2.69002593e-01,  7.74886118e-01, -3.96971623e-01,\n",
       "        2.15571722e-01, -2.12243657e-02, -1.04227420e-01, -5.45117870e-01,\n",
       "        3.80754331e-01, -1.47395657e-01,  1.29110710e-01,  1.46865551e-01,\n",
       "        5.03817368e-01, -6.30869728e-01,  1.75694613e-01,  2.65303931e-01,\n",
       "        1.93827085e-01,  2.08633440e-01,  2.55445090e-01,  7.09534804e-01,\n",
       "       -7.39535251e-02, -3.69983507e-02, -3.25559751e-02, -9.79925218e-02,\n",
       "       -2.47757618e-01,  3.12257867e-01,  2.56183487e-01,  9.86885868e-03,\n",
       "       -9.21846583e-02,  2.29666494e-01,  4.66277892e-02,  1.20198106e-01,\n",
       "        3.01208535e-01,  5.71597272e-01, -8.78316162e-02, -1.57312252e-02,\n",
       "       -5.01054242e-01,  5.32470106e-01,  4.36618567e-01, -6.77547250e+00,\n",
       "        4.31706801e+00,  2.92643891e+00,  1.35475464e-01,  8.85105295e-02,\n",
       "        1.24246571e-01,  1.19801867e-01,  0.00000000e+00,  4.40222606e-01,\n",
       "       -2.08446442e-01,  2.69947197e-01, -3.36889308e-02,  2.96304309e-01,\n",
       "        1.56115182e-01,  8.96906332e-03,  6.64587696e-03,  3.82535296e-01,\n",
       "       -7.17276445e-02,  3.17077537e-01, -1.59850758e-01,  1.88483281e-01,\n",
       "        1.83765814e-01,  9.57853359e-02,  1.48931484e+00, -4.26634756e-01,\n",
       "       -5.94645657e-01,  6.17409982e-01, -7.43828088e-02, -7.49927418e-02,\n",
       "        3.38202172e-01,  7.48773651e-02,  2.00434120e-01,  5.48991261e-02,\n",
       "       -2.00378352e-01, -9.26092328e-01,  2.28084203e-01,  3.23799576e-01,\n",
       "        2.88962952e-01,  2.12109701e-01,  2.63622723e-01,  7.75476055e-02,\n",
       "        2.88234588e-01, -9.39783203e-03, -1.04478236e-01,  2.93675911e-01,\n",
       "        4.33047521e-01, -1.98685612e-01, -2.09381614e-01,  1.17605925e-01,\n",
       "        3.25448210e-01,  7.69480478e-01, -3.98220771e-02, -2.61623970e-01,\n",
       "       -1.21378424e-01, -2.47324527e-02,  2.14549647e-02,  8.40891251e-01,\n",
       "       -2.48200907e-01, -2.45536049e-01,  1.70990150e-01,  2.89880743e-02,\n",
       "        1.62542968e-01,  5.56106485e-02,  2.95438638e-01,  2.43689306e-01,\n",
       "        1.40438476e-03,  1.09355623e-01, -8.35535347e-02,  7.63710119e-02,\n",
       "        1.20767640e-01,  4.98145708e-01,  5.11848880e-01,  6.40967062e-01,\n",
       "        8.79909346e-01, -2.35967838e+00,  2.96841812e-01, -7.16820105e-01,\n",
       "        2.50791666e-01,  3.59610664e-01,  5.74452205e-01, -1.36406215e-01,\n",
       "        3.54641519e-02,  8.79338799e-02,  1.56389752e-01,  1.62078355e-01,\n",
       "        8.13113509e-02,  1.21805024e-01,  1.08562266e-01, -1.49104134e-01,\n",
       "        2.54414949e-01,  8.53380674e-02,  2.67093939e-01, -1.38812524e-01,\n",
       "       -4.11045636e-02,  2.62625129e-01,  2.46513866e-01, -4.11045636e-02,\n",
       "        2.40992251e-01,  2.68146743e-01,  3.36417986e-01,  1.36174514e-01,\n",
       "       -4.55806907e-03,  0.00000000e+00,  3.67318587e-01,  3.84227158e-01,\n",
       "        3.04694710e-01,  6.14715290e-02, -8.48112910e-02, -2.84430887e-01,\n",
       "       -2.80435374e-01, -1.54176336e-01,  6.98618230e-02,  9.14947004e-02,\n",
       "        1.71130428e-01,  5.27195867e-01,  4.18926324e-02,  7.11791398e-02,\n",
       "       -3.50543824e-01,  1.77806094e-01,  3.73632983e-01, -8.34046462e-02,\n",
       "        1.88727228e-01, -1.13082493e-01,  3.92389697e-01,  1.38265775e-01,\n",
       "        4.31703365e-02,  2.86598319e-01, -1.51848437e-01,  6.81676679e-02,\n",
       "        5.51715200e-01,  7.67821183e-03,  2.39771387e-02,  4.36379080e-01,\n",
       "       -3.33616092e-04, -3.20560014e-02,  5.00424048e-01,  8.43819097e-02,\n",
       "       -2.05120576e-01, -8.68532258e-02,  1.71752404e-01,  2.72685975e-01,\n",
       "        2.31187944e-01,  4.09211415e-02, -4.24986727e-01,  3.11555713e-01,\n",
       "        7.23818933e-01, -1.83274629e-01,  3.90872129e-01,  1.79779438e-01,\n",
       "        9.47289363e-02, -3.53414390e-01,  1.56068318e-01,  3.90872129e-01,\n",
       "       -2.59987971e-02,  1.03161099e-01, -1.46017878e-01,  2.62479274e-01,\n",
       "        1.24673458e-02,  3.39105689e-01, -1.46017878e-01,  3.95240202e-01,\n",
       "       -2.74569884e-02, -1.05303939e-01,  1.24673458e-02, -1.50460558e-02,\n",
       "        3.12940158e-02,  3.22857729e-01,  1.50913817e-01,  1.42803460e-01,\n",
       "        1.03039707e-01,  2.46369681e-02,  4.66404792e-02,  1.50913817e-01,\n",
       "        1.42803460e-01,  1.03039707e-01,  7.12774472e-02,  5.36830321e-01,\n",
       "        1.42803460e-01,  1.03039707e-01, -1.57233763e-01, -8.61854846e-02,\n",
       "        2.46369681e-02,  2.90059726e-01, -3.85916503e-01,  1.72238865e-01,\n",
       "       -1.55032559e-01, -1.51547651e-01,  8.91207762e-02,  5.71607257e-02,\n",
       "        1.38383354e-01,  3.17710920e-01,  1.71040254e-01, -1.55032559e-01,\n",
       "       -1.51547651e-01,  8.91207762e-02,  5.71607257e-02,  1.38383354e-01,\n",
       "        1.21370728e-01,  1.97538802e-01, -1.78975095e-01,  2.38033429e-03,\n",
       "        1.20243151e-01,  2.06675121e-01,  3.17710920e-01, -1.43604593e-01,\n",
       "        4.17283198e-01,  3.17235839e-01, -1.22880013e-01])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8312358825427557\n",
      "recall:  0.7108569285580348\n",
      "precision:  0.7037312047521812\n",
      "f1 score:  0.7072761194029851\n"
     ]
    }
   ],
   "source": [
    "# try the decision tree model\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_continuous_categorical, Y_target, test_size=0.3, random_state=0)\n",
    "\n",
    "# use the default parameters\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# get the accuracy, recall, precision, f1 score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('recall: ', recall_score(y_test, y_pred))\n",
    "print('precision: ', precision_score(y_test, y_pred))\n",
    "print('f1 score: ', f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4'], dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed[:,-3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_1.13_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
