{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veify seveal hypothesis:\n",
    "\n",
    "1. Blacks in urban America are more likely to vote and vote for the Democratic candidates than are Blacks in rural America.\n",
    " \n",
    "2. Blacks in suburban America are more likely to vote for the Republican candidates than are Blacks in urban America.\n",
    " \n",
    "3. Whites in rural America are more likely to vote Republicans than are Whites in urban or suburban America.\n",
    " \n",
    "4. White non-voters are more likely to live in rural American than in urban America.\n",
    "\n",
    "we try two methods to verify:\n",
    "\n",
    "1. classical stat hypo-test (Binomial test, Chi-square test) \n",
    "\n",
    "2. run logit-reg, check the learned feature importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020.0\n",
      "1948.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "# import model\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "# data path\n",
    "file_path = '../data/cumulative_2022_v3_9_domain.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "column_to_variable_dict = np.load('../data/column_to_variable_dict.npy', allow_pickle=True).item()\n",
    "variable_to_column_dict = np.load('../data/variable_to_column_dict.npy', allow_pickle=True).item()\n",
    "\n",
    "value_label_dict = np.load('../data/value_labels.npy', allow_pickle=True).item()\n",
    "\n",
    "# check the \"Year\" column's max and min value\n",
    "print(data['Year'].max())\n",
    "print(data['Year'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples who not vote :  9809\n",
      "number of samples who vote :  27704\n",
      "number of samples who vote case DK :  0\n"
     ]
    }
   ],
   "source": [
    "target_variable = 'Voted'\n",
    "\n",
    "'''Voted  {0.0: '0. DK; NA; no Post IW; refused to say if voted;', 1.0: '1. No, did not vote', 2.0: '2. Yes, voted'}'''\n",
    "\n",
    "# filter out the samples with missing value of the target variable,drop the index\n",
    "data_new = data[data[target_variable].notnull()]\n",
    "# filter out the samples with target variable value = 0, count the number of samples whose target variable value = 0, 1 or 2\n",
    "\n",
    "\n",
    "year_threshold = 1982\n",
    "\n",
    "# folder_name = folder_name + '/'+ str(year_threshold)+ '/'\n",
    "\n",
    "# filter out the samples whose year > year_threshold\n",
    "data_new = data_new[data_new['Year'] > year_threshold]\n",
    "\n",
    "print('number of samples who not vote : ', len(data_new[data_new[target_variable] == 1]))\n",
    "print('number of samples who vote : ', len(data_new[data_new[target_variable] == 2]))\n",
    "print('number of samples who vote case DK : ', len(data_new[data_new[target_variable] == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Stat Method (hypo-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def stat_ratio(group,group_name=None):\n",
    "\n",
    "    # compute the length and the raios of voter/non-voter, vote_D, vote_R\n",
    "\n",
    "    ratio_dict = {}\n",
    "\n",
    "    if group_name is not None:\n",
    "\n",
    "        ratio_dict['name']=group_name\n",
    "    group_size =   len(group)\n",
    "    # print(group_size)\n",
    "    voter_size = len(group[group['Voted'] == 2])\n",
    "\n",
    "    ratio_dict['voter_ratio'] = voter_size/group_size\n",
    "    ratio_dict['non_voter_ratio'] = len(group[group['Voted'] == 1])/group_size\n",
    "    ratio_dict['vote_D_ratio'] = len(group[group['Voted_party'] == 1.0])/voter_size\n",
    "    ratio_dict['vote_R_ratio'] = len(group[group['Voted_party'] == 2.0])/voter_size\n",
    "\n",
    "    ratio_dict['group_size'] = group_size\n",
    "    ratio_dict['voter_size'] = voter_size\n",
    "    ratio_dict['non_voter_size'] =  len(group[group['Voted'] == 1])\n",
    "    ratio_dict['vote_D_size'] = len(group[group['Voted_party'] == 1.0])\n",
    "    ratio_dict['vote_R_size'] = len(group[group['Voted_party'] == 2.0])\n",
    "\n",
    "\n",
    "\n",
    "    return ratio_dict\n",
    "\n",
    "def stat_ratio_by_period(group,group_name=None):\n",
    "    \n",
    "    # compute the stat_ratio by cummulative years\n",
    "    \"\"\"problem: some group size are zero\"\"\"\n",
    "\n",
    "    result_dict = {}\n",
    "    result_dict['name']=group_name\n",
    "    result_dict['total_stat'] =  stat_ratio(group,group_name)\n",
    "    print(result_dict['total_stat'])\n",
    "\n",
    "    result_dict['period_stat'] = []\n",
    "    period_list = [(1983,2011),(2012,2020)]\n",
    "\n",
    "    for period in period_list:\n",
    "        start_year = period[0]\n",
    "        end_year = period[1]\n",
    "        sub_group = group[(group['Year'] <= end_year) & (group['Year'] >=start_year)]\n",
    "        result_dict['period_stat'].append(stat_ratio(sub_group,group_name))\n",
    "    return result_dict\n",
    "\n",
    "# get the black group and subgroup of Urban, Suburban and Rural\n",
    "black_group = data_new[data_new['Race7'] == 2]\n",
    "black_with_urbanism_group = black_group[black_group['urbanism'].notnull()]\n",
    "black_no_urbanism_group = black_group[black_group['urbanism'].isnull()]\n",
    "\n",
    "black_urban_group = black_group[black_group['urbanism']==1]\n",
    "black_suburban_group = black_group[black_group['urbanism']==2]\n",
    "black_rural_group = black_group[black_group['urbanism']==3]\n",
    "\n",
    "\n",
    "# black_ratio_dict =  stat_ratio_by_period(black_group, 'black_group')\n",
    "# black_with_urbanism_dict =  stat_ratio_by_period(black_with_urbanism_group, 'black_with_urbanism_group')        \n",
    "# black_no_urbanism_dict =  stat_ratio_by_period(black_no_urbanism_group, 'black_no_urbanism_group')        \n",
    "\n",
    "# black_urban_ratio_dict =  stat_ratio_by_period(black_urban_group, 'black_urban_group')\n",
    "# black_suburban_ratio_dict =  stat_ratio_by_period(black_suburban_group, 'black_suburban_group')\n",
    "# black_rural_ratio_dict =  stat_ratio_by_period(black_rural_group, 'black_rural_group')\n",
    "\n",
    "\n",
    "black_ratio_dict =  stat_ratio(black_group, 'black_group')\n",
    "black_with_urbanism_dict =  stat_ratio(black_with_urbanism_group, 'black_with_urbanism_group')        \n",
    "black_no_urbanism_dict =  stat_ratio(black_no_urbanism_group, 'black_no_urbanism_group')        \n",
    "\n",
    "black_urban_ratio_dict =  stat_ratio(black_urban_group, 'black_urban_group')\n",
    "black_suburban_ratio_dict =  stat_ratio(black_suburban_group, 'black_suburban_group')\n",
    "black_rural_ratio_dict =  stat_ratio(black_rural_group, 'black_rural_group')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>South</th>\n",
       "      <th>region</th>\n",
       "      <th>racial_composition_nbhood</th>\n",
       "      <th>racial_composition_gradeSchool</th>\n",
       "      <th>racial_composition_juniorHigh</th>\n",
       "      <th>racial_composition_highSchool</th>\n",
       "      <th>racial_composition_shops</th>\n",
       "      <th>racial_composition_friends</th>\n",
       "      <th>length_residence_home</th>\n",
       "      <th>...</th>\n",
       "      <th>interest_pubAffair</th>\n",
       "      <th>therm_DemParty</th>\n",
       "      <th>therm_RepParty</th>\n",
       "      <th>Voted_party</th>\n",
       "      <th>Voted_D_R</th>\n",
       "      <th>Family_income</th>\n",
       "      <th>occupation</th>\n",
       "      <th>occupation14</th>\n",
       "      <th>occupation71</th>\n",
       "      <th>home_ownership</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1948.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1948.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1948.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1948.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1948.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68194</th>\n",
       "      <td>2020.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68197</th>\n",
       "      <td>2020.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68201</th>\n",
       "      <td>2020.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68210</th>\n",
       "      <td>2020.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68214</th>\n",
       "      <td>2020.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6960 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Year  South  region  racial_composition_nbhood  \\\n",
       "10     1948.0    NaN     NaN                        NaN   \n",
       "37     1948.0    NaN     NaN                        NaN   \n",
       "44     1948.0    NaN     NaN                        NaN   \n",
       "60     1948.0    NaN     NaN                        NaN   \n",
       "62     1948.0    NaN     NaN                        NaN   \n",
       "...       ...    ...     ...                        ...   \n",
       "68194  2020.0    1.0     3.0                        NaN   \n",
       "68197  2020.0    2.0     2.0                        NaN   \n",
       "68201  2020.0    1.0     3.0                        NaN   \n",
       "68210  2020.0    1.0     3.0                        NaN   \n",
       "68214  2020.0    2.0     1.0                        NaN   \n",
       "\n",
       "       racial_composition_gradeSchool  racial_composition_juniorHigh  \\\n",
       "10                                NaN                            NaN   \n",
       "37                                NaN                            NaN   \n",
       "44                                NaN                            NaN   \n",
       "60                                NaN                            NaN   \n",
       "62                                NaN                            NaN   \n",
       "...                               ...                            ...   \n",
       "68194                             NaN                            NaN   \n",
       "68197                             NaN                            NaN   \n",
       "68201                             NaN                            NaN   \n",
       "68210                             NaN                            NaN   \n",
       "68214                             NaN                            NaN   \n",
       "\n",
       "       racial_composition_highSchool  racial_composition_shops  \\\n",
       "10                               NaN                       NaN   \n",
       "37                               NaN                       NaN   \n",
       "44                               NaN                       NaN   \n",
       "60                               NaN                       NaN   \n",
       "62                               NaN                       NaN   \n",
       "...                              ...                       ...   \n",
       "68194                            NaN                       NaN   \n",
       "68197                            NaN                       NaN   \n",
       "68201                            NaN                       NaN   \n",
       "68210                            NaN                       NaN   \n",
       "68214                            NaN                       NaN   \n",
       "\n",
       "       racial_composition_friends  length_residence_home  ...  \\\n",
       "10                            NaN                    NaN  ...   \n",
       "37                            NaN                    NaN  ...   \n",
       "44                            NaN                    NaN  ...   \n",
       "60                            NaN                    NaN  ...   \n",
       "62                            NaN                    NaN  ...   \n",
       "...                           ...                    ...  ...   \n",
       "68194                         NaN                    NaN  ...   \n",
       "68197                         NaN                    1.0  ...   \n",
       "68201                         NaN                    2.0  ...   \n",
       "68210                         NaN                    1.0  ...   \n",
       "68214                         NaN                    2.0  ...   \n",
       "\n",
       "       interest_pubAffair  therm_DemParty  therm_RepParty  Voted_party  \\\n",
       "10                    NaN             NaN             NaN          NaN   \n",
       "37                    NaN             NaN             NaN          NaN   \n",
       "44                    NaN             NaN             NaN          NaN   \n",
       "60                    NaN             NaN             NaN          1.0   \n",
       "62                    NaN             NaN             NaN          NaN   \n",
       "...                   ...             ...             ...          ...   \n",
       "68194                 NaN             NaN             NaN          NaN   \n",
       "68197                 NaN            60.0            15.0          NaN   \n",
       "68201                 NaN            60.0            15.0          1.0   \n",
       "68210                 NaN             0.0             0.0          NaN   \n",
       "68214                 NaN            50.0             5.0          2.0   \n",
       "\n",
       "       Voted_D_R  Family_income  occupation  occupation14  occupation71  \\\n",
       "10           NaN            3.0         NaN           NaN           NaN   \n",
       "37           NaN            1.0         NaN           NaN           NaN   \n",
       "44           NaN            2.0         NaN           NaN           NaN   \n",
       "60           1.0            3.0         NaN           NaN           NaN   \n",
       "62           NaN            2.0         NaN           NaN           NaN   \n",
       "...          ...            ...         ...           ...           ...   \n",
       "68194        NaN            1.0         NaN           NaN           NaN   \n",
       "68197        NaN            2.0         NaN           NaN           NaN   \n",
       "68201        1.0            1.0         NaN           NaN           NaN   \n",
       "68210        NaN            1.0         NaN           NaN           NaN   \n",
       "68214        2.0            1.0         NaN           NaN           NaN   \n",
       "\n",
       "       home_ownership  \n",
       "10                NaN  \n",
       "37                NaN  \n",
       "44                NaN  \n",
       "60                NaN  \n",
       "62                NaN  \n",
       "...               ...  \n",
       "68194             2.0  \n",
       "68197             2.0  \n",
       "68201             2.0  \n",
       "68210             2.0  \n",
       "68214             1.0  \n",
       "\n",
       "[6960 rows x 119 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Blacks in urban America are more likely to vote and vote for the Democratic candidates than are Blacks in rural America.\"\"\"\n",
    "def binom_test(group_dict, sub_group_dict):\n",
    "    # apply the binomial test to test the difference between given groups\n",
    "\n",
    "def chi2_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binom_test(group_dict, sub_group_dict):\n",
    "    # apply the binomial test to test the difference between given groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.908200308233164, (5.834757834757835, 0.015712572311655546))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# 示例数据生成\n",
    "np.random.seed(42) # 确保结果可复现\n",
    "\n",
    "# 假设我们有两个地区的黑人选民投票数据，urban（城市）和 rural（农村）\n",
    "# 假设城市有100个样本，农村有100个样本\n",
    "# 假定城市中有75人投票给了民主党，农村中有60人投票给了民主党\n",
    "\n",
    "# 投票数据模拟\n",
    "urban_votes = np.random.binomial(1, 0.75, 100) # 城市，75%投给民主党\n",
    "rural_votes = np.random.binomial(1, 0.60, 100) # 农村，60%投给民主党\n",
    "\n",
    "# 二项检验 - 检验城市地区投给民主党的比例是否显著不同于0.75\n",
    "binom_test_urban = stats.binom_test(urban_votes.sum(), n=100, p=0.75, alternative='two-sided')\n",
    "\n",
    "# 卡方检验 - 比较城市和农村投票给民主党的比例差异\n",
    "# 构建列联表\n",
    "contingency_table = np.array([[urban_votes.sum(), 100 - urban_votes.sum()],\n",
    "                              [rural_votes.sum(), 100 - rural_votes.sum()]])\n",
    "\n",
    "chi2_test = stats.chi2_contingency(contingency_table)[:2] # 取卡方统计量和p值\n",
    "\n",
    "binom_test_urban, chi2_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'pvalue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbinom_test_urban\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpvalue\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'pvalue'"
     ]
    }
   ],
   "source": [
    "binom_test_urban.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value = utils.missing_value_analysis(data)\n",
    "\n",
    "threshold_list = [0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "must_include_list = ['urbanism']\n",
    "\n",
    "used_features, not_used_features, folder_name = utils.feature_filter(data, threshold_list,column_to_variable_dict, must_include_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing value of the non-feature variable: \n",
      "Voted                0.091551\n",
      "Registered_voted     0.218061\n",
      "Voted_party          0.536483\n",
      "Vote_Nonvote_Pres    0.377067\n",
      "Race3                0.024874\n",
      "Race4                0.024874\n",
      "Race7                0.024068\n",
      "religion             0.007065\n",
      "Year                 0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# use the used features to filter out the data\n",
    "\n",
    "# set the target variable set and index variable set, these variables will not be used for training\n",
    "\n",
    "target_variable_list = ['Voted','Registered_voted','Voted_party','Vote_Nonvote_Pres']\n",
    "\n",
    "race_variable_list = ['Race3','Race4','Race7']\n",
    "\n",
    "religion_variable_list = ['religion']\n",
    "\n",
    "index_variable_list = ['Year', ]\n",
    "\n",
    "state_variable_list = ['State']\n",
    "\n",
    "non_feature_list = target_variable_list +  race_variable_list + religion_variable_list + index_variable_list\n",
    "\n",
    "# check the missing ratio of the target variable\n",
    "print('missing value of the non-feature variable: ')\n",
    "print(data[non_feature_list].isnull().sum() / len(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples who not vote :  17790\n",
      "number of samples who vote :  44188\n",
      "number of samples who vote case DK :  0\n",
      "(61978, 118)\n",
      "(37513, 118)\n",
      "number of numerical features:  9\n",
      "number of categorical features:  40\n",
      "numerical features list: ['therm_ChrFundament', 'therm_hispanics', 'therm_RepParty', 'therm_DemParty', 'therm_Whites', 'therm_liberals', 'therm_conservatives', 'therm_Blacks', 'Age']\n"
     ]
    }
   ],
   "source": [
    "target_variable = 'Voted'\n",
    "\n",
    "'''Voted  {0.0: '0. DK; NA; no Post IW; refused to say if voted;', 1.0: '1. No, did not vote', 2.0: '2. Yes, voted'}'''\n",
    "\n",
    "# filter out the samples with missing value of the target variable,drop the index\n",
    "data_new = data[data[target_variable].notnull()]\n",
    "# filter out the samples with target variable value = 0, count the number of samples whose target variable value = 0, 1 or 2\n",
    "print('number of samples who not vote : ', len(data_new[data_new[target_variable] == 1]))\n",
    "print('number of samples who vote : ', len(data_new[data_new[target_variable] == 2]))\n",
    "print('number of samples who vote case DK : ', len(data_new[data_new[target_variable] == 0]))\n",
    "\n",
    "data_new = data_new[data_new[target_variable] != 0]\n",
    "data_new = data_new.reset_index(drop=True)\n",
    "print(data_new.shape)\n",
    "\n",
    "year_threshold = 1982\n",
    "\n",
    "folder_name = folder_name + '/'+ str(year_threshold)+ '/'\n",
    "\n",
    "# filter out the samples whose year > year_threshold\n",
    "data_new = data_new[data_new['Year'] > year_threshold]\n",
    "data_new = data_new.reset_index(drop=True)\n",
    "print(data_new.shape)\n",
    "\n",
    "\n",
    "numerical_feature_list, categorical_feature_list = utils.feature_type_analysis(data, used_features, non_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples of White:  26879\n",
      "number of samples of Black:  4655\n",
      "number of samples of Asian:  696\n",
      "number of samples of American_Indian:  428\n",
      "number of samples of Hispanic:  3849\n",
      "number of samples of Other:  725\n",
      "number of samples of Protestant:  18623\n",
      "number of samples of Catholic:  8686\n",
      "number of samples of Jewish:  746\n",
      "number of samples of Other:  9319\n"
     ]
    }
   ],
   "source": [
    "# slipt the group by race and religion\n",
    "\n",
    "data_race7_dict = utils.group_split_race7(data_new) \n",
    "data_religion_dict = utils.group_split_religon(data_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26879, 50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average accuracy:  0.857844545957918\n",
      "average recall:  0.5902974140840849\n",
      "average precision:  0.7661200618794732\n",
      "average f1 score:  0.6667050852190337\n",
      "average roc auc score:  0.766497464955573\n",
      "(4655, 50)\n",
      "average accuracy:  0.8161117078410312\n",
      "average recall:  0.5658855535320377\n",
      "average precision:  0.7169315645735456\n",
      "average f1 score:  0.6322157806803577\n",
      "average roc auc score:  0.7395545146375999\n",
      "(696, 50)\n",
      "average accuracy:  0.794563206577595\n",
      "average recall:  0.5269227970084763\n",
      "average precision:  0.6782051282051281\n",
      "average f1 score:  0.5916610997462569\n",
      "average roc auc score:  0.7131760024691828\n",
      "(428, 50)\n",
      "average accuracy:  0.7569630642954857\n",
      "average recall:  0.6892585321655089\n",
      "average precision:  0.7031446351802303\n",
      "average f1 score:  0.6942463412974214\n",
      "average roc auc score:  0.7465784968500943\n",
      "(3849, 50)\n",
      "average accuracy:  0.7843598534105687\n",
      "average recall:  0.6491549672106925\n",
      "average precision:  0.7342200353076385\n",
      "average f1 score:  0.6881180998862091\n",
      "average roc auc score:  0.7564110402840791\n",
      "(725, 50)\n",
      "average accuracy:  0.8248275862068966\n",
      "average recall:  0.5311111111111111\n",
      "average precision:  0.6606016991504247\n",
      "average f1 score:  0.5866515286675857\n",
      "average roc auc score:  0.7231942393131064\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# start from all-clear case:  further filter out the samples with missing value of the used features\n",
    "\n",
    "group = 'race'\n",
    "\n",
    "for group_cat in data_race7_dict.keys():\n",
    "\n",
    "    data_group = data_race7_dict[group_cat]\n",
    "\n",
    "    X_categorical_transformed, X_continuous_transformed, Y_target, enc_categorical_feature_list = utils.feature_process(data_group, numerical_feature_list, categorical_feature_list, target_variable,value_label_dict)\n",
    "\n",
    "    X_continuous_categorical = np.concatenate((X_continuous_transformed, X_categorical_transformed), axis=1)\n",
    "\n",
    "    model = LogisticRegression(l1_ratio = 0.5, max_iter = 500, solver = 'saga', penalty = 'elasticnet')\n",
    "\n",
    "    accuracy_list, recall_list, precision_list, f1_list, roc_auc_list, importance_list = utils.cross_validation(X_continuous_categorical, Y_target, model, k = 5)\n",
    "\n",
    "    print('average accuracy: ', np.mean(accuracy_list))\n",
    "    print('average recall: ', np.mean(recall_list))\n",
    "    print('average precision: ', np.mean(precision_list))\n",
    "    print('average f1 score: ', np.mean(f1_list))\n",
    "    print('average roc auc score: ', np.mean(roc_auc_list))\n",
    "\n",
    "    # build the feature importance dataframe\n",
    "    feature_importance = pd.DataFrame({'feature': numerical_feature_list + enc_categorical_feature_list, 'importance': np.mean(importance_list, axis=0)})\n",
    "\n",
    "    top_15_positive = feature_importance.sort_values('importance', ascending = False).head(15)\n",
    "    top_15_negative = feature_importance.sort_values('importance', ascending = True).head(15)\n",
    "\n",
    "    # build a folder to save the results\n",
    "    sub_folder_name = folder_name + group + '/' + group_cat + '/'\n",
    "    if not os.path.exists(sub_folder_name):\n",
    "        os.makedirs(sub_folder_name)\n",
    "\n",
    "    feature_importance.to_csv(sub_folder_name + 'feature_importance.csv', index = False)\n",
    "    top_15_positive.to_csv(sub_folder_name + 'top_15_voter.csv', index = False)\n",
    "    top_15_negative.to_csv(sub_folder_name + 'top_15_non_voter.csv', index = False)\n",
    "\n",
    "    # save the mean of the metrics\n",
    "    metrics = pd.DataFrame({'accuracy': np.mean(accuracy_list), 'recall': np.mean(recall_list), 'precision': np.mean(precision_list), 'f1': np.mean(f1_list), 'roc_auc': np.mean(roc_auc_list)}, index = [0])\n",
    "    metrics.to_csv(sub_folder_name + 'metrics.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18623, 50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average accuracy:  0.8480371686647107\n",
      "average recall:  0.5845162305310698\n",
      "average precision:  0.7658612660156082\n",
      "average f1 score:  0.6629330300518849\n",
      "average roc auc score:  0.7615593240322965\n",
      "(8686, 50)\n",
      "average accuracy:  0.8411231750839543\n",
      "average recall:  0.5523585929784071\n",
      "average precision:  0.7290241880938019\n",
      "average f1 score:  0.6281950296782736\n",
      "average roc auc score:  0.7431751796911741\n",
      "(746, 50)\n",
      "average accuracy:  0.8780492170022371\n",
      "average recall:  0.3091754385964912\n",
      "average precision:  0.5468686868686868\n",
      "average f1 score:  0.39288689611270255\n",
      "average roc auc score:  0.6361456378935157\n",
      "(9319, 50)\n",
      "average accuracy:  0.8415062695960873\n",
      "average recall:  0.6603457886651961\n",
      "average precision:  0.7736979566419017\n",
      "average f1 score:  0.7124332614824447\n",
      "average roc auc score:  0.7893140910932284\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# start from all-clear case:  further filter out the samples with missing value of the used features\n",
    "\n",
    "group = 'religon'\n",
    "\n",
    "for group_cat in data_religion_dict.keys():\n",
    "\n",
    "    data_group = data_religion_dict[group_cat]\n",
    "\n",
    "    X_categorical_transformed, X_continuous_transformed, Y_target, enc_categorical_feature_list = utils.feature_process(data_group, numerical_feature_list, categorical_feature_list, target_variable,value_label_dict)\n",
    "\n",
    "    X_continuous_categorical = np.concatenate((X_continuous_transformed, X_categorical_transformed), axis=1)\n",
    "\n",
    "    model = LogisticRegression(l1_ratio = 0.5, max_iter = 500, solver = 'saga', penalty = 'elasticnet')\n",
    "\n",
    "    accuracy_list, recall_list, precision_list, f1_list, roc_auc_list, importance_list = utils.cross_validation(X_continuous_categorical, Y_target, model, k = 5)\n",
    "\n",
    "    print('average accuracy: ', np.mean(accuracy_list))\n",
    "    print('average recall: ', np.mean(recall_list))\n",
    "    print('average precision: ', np.mean(precision_list))\n",
    "    print('average f1 score: ', np.mean(f1_list))\n",
    "    print('average roc auc score: ', np.mean(roc_auc_list))\n",
    "\n",
    "    # build the feature importance dataframe\n",
    "    feature_importance = pd.DataFrame({'feature': numerical_feature_list + enc_categorical_feature_list, 'importance': np.mean(importance_list, axis=0)})\n",
    "\n",
    "    top_15_positive = feature_importance.sort_values('importance', ascending = False).head(15)\n",
    "    top_15_negative = feature_importance.sort_values('importance', ascending = True).head(15)\n",
    "\n",
    "    # build a folder to save the results\n",
    "    sub_folder_name = folder_name + group + '/' + group_cat + '/'\n",
    "    if not os.path.exists(sub_folder_name):\n",
    "        os.makedirs(sub_folder_name)\n",
    "\n",
    "    feature_importance.to_csv(sub_folder_name + 'feature_importance.csv', index = False)\n",
    "    top_15_positive.to_csv(sub_folder_name + 'top_15_voter.csv', index = False)\n",
    "    top_15_negative.to_csv(sub_folder_name + 'top_15_non_voter.csv', index = False)\n",
    "\n",
    "    # save the mean of the metrics\n",
    "    metrics = pd.DataFrame({'accuracy': np.mean(accuracy_list), 'recall': np.mean(recall_list), 'precision': np.mean(precision_list), 'f1': np.mean(f1_list), 'roc_auc': np.mean(roc_auc_list)}, index = [0])\n",
    "    metrics.to_csv(sub_folder_name + 'metrics.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_1.13_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
