{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance analysis acroos intend-to-vote-but-final-vote groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020.0\n",
      "1948.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "# import model\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# data path\n",
    "file_path = '../data/cumulative_2022_v3_9_domain.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "column_to_variable_dict = np.load('../data/column_to_variable_dict.npy', allow_pickle=True).item()\n",
    "variable_to_column_dict = np.load('../data/variable_to_column_dict.npy', allow_pickle=True).item()\n",
    "\n",
    "value_label_dict = np.load('../data/value_labels.npy', allow_pickle=True).item()\n",
    "\n",
    "# check the \"Year\" column's max and min value\n",
    "print(data['Year'].max())\n",
    "print(data['Year'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31087"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_variable = 'Voted'\n",
    "\n",
    "'''Voted  {0.0: '0. DK; NA; no Post IW; refused to say if voted;', 1.0: '1. No, did not vote', 2.0: '2. Yes, voted'}'''\n",
    "\n",
    "data_new = data[data[target_variable].notnull()]\n",
    "\n",
    "sub_target_variable = 'Pre_election_inten_vote'\n",
    "\n",
    "'''Pre_election_inten_vote  {0.0: '0. DK (1964 only); NA; no Pre IW; DK/NA/RF (1952', 1.0: '1. Democratic candidate (with or without qualifications,', 2.0: '2. Republican candidate (with or without qualifications,', 3.0: '3. Undecided; DK (except 1964)', 4.0: \"4. R does not intend to vote (incl. 'no, qualified' if\", 9.0: '9. Other candidate'}\n",
    "'''\n",
    "\n",
    "\n",
    "# the total valid number of samples, who intend to vote for Democratic candidate or Republican candidate\n",
    "data_new = data_new[(data_new[sub_target_variable]== 1) | (data_new[sub_target_variable] == 2)]\n",
    "\n",
    "len(data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value = utils.missing_value_analysis(data)\n",
    "\n",
    "threshold_list = [0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# must_include_list = ['urbanism']\n",
    "must_include_list = None\n",
    "\n",
    "\n",
    "folder_name = '../data/non-voter-feature-analysis/'\n",
    "\n",
    "used_features, not_used_features, folder_name = utils.feature_filter(data, threshold_list,column_to_variable_dict, folder_name, must_include_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing value of the non-feature variable: \n",
      "Voted                      0.000000\n",
      "Registered_voted           0.016666\n",
      "Voted_party                0.058205\n",
      "Vote_Nonvote_Pres          0.003811\n",
      "Race3                      0.001832\n",
      "Race4                      0.001832\n",
      "Race7                      0.001656\n",
      "religion                   0.000835\n",
      "Year                       0.000000\n",
      "Pre_election_inten_vote    0.000000\n",
      "State                      0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# use the used features to filter out the data\n",
    "\n",
    "# set the target variable set and index variable set, these variables will not be used for training\n",
    "\n",
    "target_variable_list = ['Voted','Registered_voted','Voted_party','Vote_Nonvote_Pres']\n",
    "\n",
    "race_variable_list = ['Race3','Race4','Race7']\n",
    "\n",
    "religion_variable_list = ['religion']\n",
    "\n",
    "index_variable_list = ['Year', ]\n",
    "\n",
    "not_used_features = ['Pre_election_inten_vote']\n",
    "\n",
    "state_variable_list = ['State']\n",
    "\n",
    "non_feature_list = target_variable_list +  race_variable_list + religion_variable_list + index_variable_list + not_used_features + state_variable_list\n",
    "\n",
    "# check the missing ratio of the target variable\n",
    "print('missing value of the non-feature variable: ')\n",
    "print(data_new[non_feature_list].isnull().sum() / len(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples who not vote :  3472\n",
      "number of samples who vote :  27615\n",
      "number of samples who vote case DK :  0\n",
      "(31087, 119)\n",
      "(21820, 119)\n",
      "number of numerical features:  9\n",
      "number of categorical features:  38\n",
      "numerical features list: ['therm_ChrFundament', 'therm_hispanics', 'therm_RepParty', 'therm_DemParty', 'therm_Whites', 'therm_liberals', 'therm_conservatives', 'therm_Blacks', 'Age']\n"
     ]
    }
   ],
   "source": [
    "target_variable = 'Voted'\n",
    "\n",
    "'''Voted  {0.0: '0. DK; NA; no Post IW; refused to say if voted;', 1.0: '1. No, did not vote', 2.0: '2. Yes, voted'}'''\n",
    "\n",
    "# filter out the samples with missing value of the target variable,drop the index\n",
    "data_new = data_new[data_new[target_variable].notnull()]\n",
    "# filter out the samples with target variable value = 0, count the number of samples whose target variable value = 0, 1 or 2\n",
    "print('number of samples who not vote : ', len(data_new[data_new[target_variable] == 1]))\n",
    "print('number of samples who vote : ', len(data_new[data_new[target_variable] == 2]))\n",
    "print('number of samples who vote case DK : ', len(data_new[data_new[target_variable] == 0]))\n",
    "\n",
    "data_new = data_new[data_new[target_variable] != 0]\n",
    "data_new = data_new.reset_index(drop=True)\n",
    "print(data_new.shape)\n",
    "\n",
    "year_threshold = 1982\n",
    "\n",
    "folder_name = folder_name + '/'+ str(year_threshold)+ '/'\n",
    "\n",
    "# filter out the samples whose year > year_threshold\n",
    "data_new = data_new[data_new['Year'] > year_threshold]\n",
    "data_new = data_new.reset_index(drop=True)\n",
    "print(data_new.shape)\n",
    "\n",
    "\n",
    "numerical_feature_list, categorical_feature_list = utils.feature_type_analysis(data_new, used_features, non_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples of White:  15554\n",
      "number of samples of Black:  2823\n",
      "number of samples of Asian:  427\n",
      "number of samples of American_Indian:  220\n",
      "number of samples of Hispanic:  2215\n",
      "number of samples of Other:  483\n",
      "number of samples of Protestant:  10699\n",
      "number of samples of Catholic:  5017\n",
      "number of samples of Jewish:  496\n",
      "number of samples of Other:  5582\n"
     ]
    }
   ],
   "source": [
    "# slipt the group by race and religion\n",
    "\n",
    "data_race7_dict = utils.group_split_race7(data_new) \n",
    "data_religion_dict = utils.group_split_religon(data_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15554, 48)\n",
      "average accuracy:  0.9079339053105825\n",
      "average recall:  0.05999477630412241\n",
      "average precision:  0.5112703388565458\n",
      "average f1 score:  0.1062543131390848\n",
      "average roc auc score:  0.527024022792318\n",
      "(2823, 48)\n",
      "average accuracy:  0.8622048578422143\n",
      "average recall:  0.1749605024834128\n",
      "average precision:  0.5381654690652993\n",
      "average f1 score:  0.2601722548285574\n",
      "average roc auc score:  0.5747759509428366\n",
      "(427, 48)\n",
      "average accuracy:  0.8430369357045144\n",
      "average recall:  0.12793650793650793\n",
      "average precision:  0.22575757575757574\n",
      "average f1 score:  0.15790476190476188\n",
      "average roc auc score:  0.5395802005012532\n",
      "(220, 48)\n",
      "average accuracy:  0.7863636363636364\n",
      "average recall:  0.45195360195360196\n",
      "average precision:  0.5452380952380953\n",
      "average f1 score:  0.47257085020242917\n",
      "average roc auc score:  0.6671860891820569\n",
      "(2215, 48)\n",
      "average accuracy:  0.7959367945823927\n",
      "average recall:  0.2226070525123441\n",
      "average precision:  0.5014062659223949\n",
      "average f1 score:  0.306903660458258\n",
      "average roc auc score:  0.5839516085317278\n",
      "(483, 48)\n",
      "average accuracy:  0.8364046391752578\n",
      "average recall:  0.1456818181818182\n",
      "average precision:  0.34025974025974026\n",
      "average f1 score:  0.18845096932053457\n",
      "average roc auc score:  0.5399272612889805\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# start from all-clear case:  further filter out the samples with missing value of the used features\n",
    "\n",
    "group = 'race'\n",
    "\n",
    "for group_cat in data_race7_dict.keys():\n",
    "\n",
    "    data_group = data_race7_dict[group_cat]\n",
    "\n",
    "    X_categorical_transformed, X_continuous_transformed, Y_target, enc_categorical_feature_list = utils.feature_process(data_group, numerical_feature_list, categorical_feature_list, target_variable,value_label_dict)\n",
    "\n",
    "    X_continuous_categorical = np.concatenate((X_continuous_transformed, X_categorical_transformed), axis=1)\n",
    "\n",
    "    model = LogisticRegression(l1_ratio = 0.5, max_iter = 500, solver = 'saga', penalty = 'elasticnet')\n",
    "\n",
    "    accuracy_list, recall_list, precision_list, f1_list, roc_auc_list, importance_list = utils.cross_validation(X_continuous_categorical, Y_target, model, k = 5)\n",
    "\n",
    "    print('average accuracy: ', np.mean(accuracy_list))\n",
    "    print('average recall: ', np.mean(recall_list))\n",
    "    print('average precision: ', np.mean(precision_list))\n",
    "    print('average f1 score: ', np.mean(f1_list))\n",
    "    print('average roc auc score: ', np.mean(roc_auc_list))\n",
    "\n",
    "    # build the feature importance dataframe\n",
    "    feature_importance = pd.DataFrame({'feature': numerical_feature_list + enc_categorical_feature_list, 'importance': np.mean(importance_list, axis=0)})\n",
    "\n",
    "    top_15_positive = feature_importance.sort_values('importance', ascending = False).head(15)\n",
    "    top_15_negative = feature_importance.sort_values('importance', ascending = True).head(15)\n",
    "\n",
    "    # build a folder to save the results\n",
    "    sub_folder_name = folder_name + group + '/' + group_cat + '/'\n",
    "    if not os.path.exists(sub_folder_name):\n",
    "        os.makedirs(sub_folder_name)\n",
    "\n",
    "    feature_importance.to_csv(sub_folder_name + 'feature_importance.csv', index = False)\n",
    "    top_15_positive.to_csv(sub_folder_name + 'top_15_voter.csv', index = False)\n",
    "    top_15_negative.to_csv(sub_folder_name + 'top_15_non_voter.csv', index = False)\n",
    "\n",
    "    # save the mean of the metrics\n",
    "    metrics = pd.DataFrame({'accuracy': np.mean(accuracy_list), 'recall': np.mean(recall_list), 'precision': np.mean(precision_list), 'f1': np.mean(f1_list), 'roc_auc': np.mean(roc_auc_list)}, index = [0])\n",
    "    metrics.to_csv(sub_folder_name + 'metrics.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30757, 62)\n"
     ]
    }
   ],
   "source": [
    "    data_XY = data_new[numerical_feature_list + categorical_feature_list+[target_variable]]\n",
    "    # data_XY = data_XY[data_XY.notnull().all(axis=1)]\n",
    "    data_XY = data_XY.reset_index(drop=True)\n",
    "    print(data_XY.shape)\n",
    "\n",
    "    X_continuous = data_XY[numerical_feature_list]\n",
    "    X_categorical = data_XY[categorical_feature_list]\n",
    "    Y_target = data_XY[target_variable]\n",
    "\n",
    "    # impute + process(one-hot)  categorical features (also get the new names)\n",
    "\n",
    "    X_categorical_imp = X_categorical.fillna(-1)\n",
    "\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    enc.fit(X_categorical_imp)\n",
    "\n",
    "    X_categorical_transformed = enc.transform(X_categorical_imp).toarray()\n",
    "\n",
    "    initial_list = enc.get_feature_names().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature id:  4\n",
      "category index:  4\n",
      "feature name:  sex_orientation\n"
     ]
    }
   ],
   "source": [
    "    feature_id = int(string.split('_')[0][1:])\n",
    "    category_index = int(float(string.split('_')[1]))\n",
    "    feature_name = enc.feature_names_in_[feature_id]\n",
    "    print('feature id: ', feature_id)\n",
    "    print('category index: ', category_index)\n",
    "    print('feature name: ', feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-9.0: '-9. RF; NA; Inap',\n",
       " -8.0: '-8. DK',\n",
       " 1.0: '1. Heterosexual or straight',\n",
       " 2.0: '2. Bisexual',\n",
       " 3.0: '3. Homosexual or gay (or lesbian)'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_label_dict['sex_orientation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m new_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m string \u001b[38;5;129;01min\u001b[39;00m initial_list:\n\u001b[0;32m----> 4\u001b[0m     feature_name, category_name \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_name_category_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_label_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     new_list\u001b[38;5;241m.\u001b[39mappend((feature_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m category_name))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_list\n",
      "File \u001b[0;32m~/fang/ML-Poli-Sci/code/utils.py:211\u001b[0m, in \u001b[0;36mget_feature_name_category_name\u001b[0;34m(string, enc, value_label_dict)\u001b[0m\n\u001b[1;32m    209\u001b[0m     category_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     category_name \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_label_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcategory_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feature_name, category_name\n",
      "\u001b[0;31mKeyError\u001b[0m: 4"
     ]
    }
   ],
   "source": [
    "    new_list = []\n",
    "\n",
    "    for string in initial_list:\n",
    "        # feature_name, category_name = utils.get_feature_name_category_name(string, enc, value_label_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        new_list.append((feature_name+'_'+ category_name))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18623, 50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average accuracy:  0.8480371686647107\n",
      "average recall:  0.5845162305310698\n",
      "average precision:  0.7658612660156082\n",
      "average f1 score:  0.6629330300518849\n",
      "average roc auc score:  0.7615593240322965\n",
      "(8686, 50)\n",
      "average accuracy:  0.8411231750839543\n",
      "average recall:  0.5523585929784071\n",
      "average precision:  0.7290241880938019\n",
      "average f1 score:  0.6281950296782736\n",
      "average roc auc score:  0.7431751796911741\n",
      "(746, 50)\n",
      "average accuracy:  0.8780492170022371\n",
      "average recall:  0.3091754385964912\n",
      "average precision:  0.5468686868686868\n",
      "average f1 score:  0.39288689611270255\n",
      "average roc auc score:  0.6361456378935157\n",
      "(9319, 50)\n",
      "average accuracy:  0.8415062695960873\n",
      "average recall:  0.6603457886651961\n",
      "average precision:  0.7736979566419017\n",
      "average f1 score:  0.7124332614824447\n",
      "average roc auc score:  0.7893140910932284\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# start from all-clear case:  further filter out the samples with missing value of the used features\n",
    "\n",
    "group = 'religon'\n",
    "\n",
    "for group_cat in data_religion_dict.keys():\n",
    "\n",
    "    data_group = data_religion_dict[group_cat]\n",
    "\n",
    "    X_categorical_transformed, X_continuous_transformed, Y_target, enc_categorical_feature_list = utils.feature_process(data_group, numerical_feature_list, categorical_feature_list, target_variable,value_label_dict)\n",
    "\n",
    "    X_continuous_categorical = np.concatenate((X_continuous_transformed, X_categorical_transformed), axis=1)\n",
    "\n",
    "    model = LogisticRegression(l1_ratio = 0.5, max_iter = 500, solver = 'saga', penalty = 'elasticnet')\n",
    "\n",
    "    accuracy_list, recall_list, precision_list, f1_list, roc_auc_list, importance_list = utils.cross_validation(X_continuous_categorical, Y_target, model, k = 5)\n",
    "\n",
    "    print('average accuracy: ', np.mean(accuracy_list))\n",
    "    print('average recall: ', np.mean(recall_list))\n",
    "    print('average precision: ', np.mean(precision_list))\n",
    "    print('average f1 score: ', np.mean(f1_list))\n",
    "    print('average roc auc score: ', np.mean(roc_auc_list))\n",
    "\n",
    "    # build the feature importance dataframe\n",
    "    feature_importance = pd.DataFrame({'feature': numerical_feature_list + enc_categorical_feature_list, 'importance': np.mean(importance_list, axis=0)})\n",
    "\n",
    "    top_15_positive = feature_importance.sort_values('importance', ascending = False).head(15)\n",
    "    top_15_negative = feature_importance.sort_values('importance', ascending = True).head(15)\n",
    "\n",
    "    # build a folder to save the results\n",
    "    sub_folder_name = folder_name + group + '/' + group_cat + '/'\n",
    "    if not os.path.exists(sub_folder_name):\n",
    "        os.makedirs(sub_folder_name)\n",
    "\n",
    "    feature_importance.to_csv(sub_folder_name + 'feature_importance.csv', index = False)\n",
    "    top_15_positive.to_csv(sub_folder_name + 'top_15_voter.csv', index = False)\n",
    "    top_15_negative.to_csv(sub_folder_name + 'top_15_non_voter.csv', index = False)\n",
    "\n",
    "    # save the mean of the metrics\n",
    "    metrics = pd.DataFrame({'accuracy': np.mean(accuracy_list), 'recall': np.mean(recall_list), 'precision': np.mean(precision_list), 'f1': np.mean(f1_list), 'roc_auc': np.mean(roc_auc_list)}, index = [0])\n",
    "    metrics.to_csv(sub_folder_name + 'metrics.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_1.13_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
