{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance analysis acroos different group (race/religon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020.0\n",
      "1948.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "# import model\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# data path\n",
    "file_path = '../data/cumulative_2022_v3_9_domain.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "column_to_variable_dict = np.load('../data/column_to_variable_dict.npy', allow_pickle=True).item()\n",
    "variable_to_column_dict = np.load('../data/variable_to_column_dict.npy', allow_pickle=True).item()\n",
    "\n",
    "value_label_dict = np.load('../data/value_labels.npy', allow_pickle=True).item()\n",
    "\n",
    "# check the \"Year\" column's max and min value\n",
    "print(data['Year'].max())\n",
    "print(data['Year'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value = utils.missing_value_analysis(data)\n",
    "\n",
    "threshold_list = [0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "used_features, not_used_features, folder_name = utils.feature_filter(data, threshold_list,column_to_variable_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing value of the non-feature variable: \n",
      "Voted                0.091551\n",
      "Registered_voted     0.218061\n",
      "Voted_party          0.536483\n",
      "Vote_Nonvote_Pres    0.377067\n",
      "Race3                0.024874\n",
      "Race4                0.024874\n",
      "Race7                0.024068\n",
      "religion             0.007065\n",
      "Year                 0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# use the used features to filter out the data\n",
    "\n",
    "# set the target variable set and index variable set, these variables will not be used for training\n",
    "\n",
    "target_variable_list = ['Voted','Registered_voted','Voted_party','Vote_Nonvote_Pres']\n",
    "\n",
    "race_variable_list = ['Race3','Race4','Race7']\n",
    "\n",
    "religion_variable_list = ['religion']\n",
    "\n",
    "index_variable_list = ['Year', ]\n",
    "\n",
    "non_feature_list = target_variable_list +  race_variable_list + religion_variable_list + index_variable_list\n",
    "\n",
    "# check the missing ratio of the target variable\n",
    "print('missing value of the non-feature variable: ')\n",
    "print(data[non_feature_list].isnull().sum() / len(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples who not vote :  17790\n",
      "number of samples who vote :  44188\n",
      "number of samples who vote case DK :  0\n",
      "(61978, 118)\n",
      "(37513, 118)\n",
      "number of numerical features:  9\n",
      "number of categorical features:  39\n",
      "numerical features list: ['therm_ChrFundament', 'therm_hispanics', 'therm_RepParty', 'therm_DemParty', 'therm_Whites', 'therm_liberals', 'therm_conservatives', 'therm_Blacks', 'Age']\n"
     ]
    }
   ],
   "source": [
    "target_variable = 'Voted'\n",
    "\n",
    "'''Voted  {0.0: '0. DK; NA; no Post IW; refused to say if voted;', 1.0: '1. No, did not vote', 2.0: '2. Yes, voted'}'''\n",
    "\n",
    "# filter out the samples with missing value of the target variable,drop the index\n",
    "data_new = data[data[target_variable].notnull()]\n",
    "# filter out the samples with target variable value = 0, count the number of samples whose target variable value = 0, 1 or 2\n",
    "print('number of samples who not vote : ', len(data_new[data_new[target_variable] == 1]))\n",
    "print('number of samples who vote : ', len(data_new[data_new[target_variable] == 2]))\n",
    "print('number of samples who vote case DK : ', len(data_new[data_new[target_variable] == 0]))\n",
    "\n",
    "data_new = data_new[data_new[target_variable] != 0]\n",
    "data_new = data_new.reset_index(drop=True)\n",
    "print(data_new.shape)\n",
    "\n",
    "year_threshold = 1982\n",
    "\n",
    "folder_name = folder_name + '/'+ str(year_threshold)+ '/'\n",
    "\n",
    "# filter out the samples whose year > year_threshold\n",
    "data_new = data_new[data_new['Year'] > year_threshold]\n",
    "data_new = data_new.reset_index(drop=True)\n",
    "print(data_new.shape)\n",
    "\n",
    "\n",
    "numerical_feature_list, categorical_feature_list = utils.feature_type_analysis(data, used_features, non_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples of White:  26879\n",
      "number of samples of Black:  4655\n",
      "number of samples of Asian:  696\n",
      "number of samples of American_Indian:  428\n",
      "number of samples of Hispanic:  3849\n",
      "number of samples of Other:  725\n",
      "number of samples of Protestant:  18623\n",
      "number of samples of Catholic:  8686\n",
      "number of samples of Jewish:  746\n",
      "number of samples of Other:  9319\n"
     ]
    }
   ],
   "source": [
    "# slipt the group by race and religion\n",
    "\n",
    "data_race7_dict = utils.group_split_race7(data_new) \n",
    "data_religion_dict = utils.group_split_religon(data_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26879, 49)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average accuracy:  0.8574725290697675\n",
      "average recall:  0.5887577990088648\n",
      "average precision:  0.7656928979271854\n",
      "average f1 score:  0.6655579085581008\n",
      "average roc auc score:  0.7657272141667288\n",
      "(4655, 49)\n",
      "average accuracy:  0.8154672395273899\n",
      "average recall:  0.5643850656788532\n",
      "average precision:  0.7156280357109586\n",
      "average f1 score:  0.6308105964822539\n",
      "average roc auc score:  0.7386576631243853\n",
      "(696, 49)\n",
      "average accuracy:  0.8003186022610482\n",
      "average recall:  0.5477195654368238\n",
      "average precision:  0.6866038452245349\n",
      "average f1 score:  0.6081165096614191\n",
      "average roc auc score:  0.7235743866833566\n",
      "(428, 49)\n",
      "average accuracy:  0.7593160054719562\n",
      "average recall:  0.6892585321655089\n",
      "average precision:  0.7080707435546145\n",
      "average f1 score:  0.696432133647148\n",
      "average roc auc score:  0.7484652893029244\n",
      "(3849, 49)\n",
      "average accuracy:  0.784619255906642\n",
      "average recall:  0.648563057207304\n",
      "average precision:  0.7351665565639365\n",
      "average f1 score:  0.6881565163399478\n",
      "average roc auc score:  0.7564810499590445\n",
      "(725, 49)\n",
      "average accuracy:  0.8248275862068966\n",
      "average recall:  0.5196732026143791\n",
      "average precision:  0.6641074787281683\n",
      "average f1 score:  0.5806337306761036\n",
      "average roc auc score:  0.7193101474500615\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# start from all-clear case:  further filter out the samples with missing value of the used features\n",
    "\n",
    "group = 'race'\n",
    "\n",
    "for group_cat in data_race7_dict.keys():\n",
    "\n",
    "    data_group = data_race7_dict[group_cat]\n",
    "\n",
    "    X_categorical_transformed, X_continuous_transformed, Y_target, enc_categorical_feature_list = utils.feature_process(data_group, numerical_feature_list, categorical_feature_list, target_variable,value_label_dict)\n",
    "\n",
    "    X_continuous_categorical = np.concatenate((X_continuous_transformed, X_categorical_transformed), axis=1)\n",
    "\n",
    "    model = LogisticRegression(l1_ratio = 0.5, max_iter = 500, solver = 'saga', penalty = 'elasticnet')\n",
    "\n",
    "    accuracy_list, recall_list, precision_list, f1_list, roc_auc_list, importance_list = utils.cross_validation(X_continuous_categorical, Y_target, model, k = 5)\n",
    "\n",
    "    print('average accuracy: ', np.mean(accuracy_list))\n",
    "    print('average recall: ', np.mean(recall_list))\n",
    "    print('average precision: ', np.mean(precision_list))\n",
    "    print('average f1 score: ', np.mean(f1_list))\n",
    "    print('average roc auc score: ', np.mean(roc_auc_list))\n",
    "\n",
    "    # build the feature importance dataframe\n",
    "    feature_importance = pd.DataFrame({'feature': numerical_feature_list + enc_categorical_feature_list, 'importance': np.mean(importance_list, axis=0)})\n",
    "\n",
    "    top_15_positive = feature_importance.sort_values('importance', ascending = False).head(15)\n",
    "    top_15_negative = feature_importance.sort_values('importance', ascending = True).head(15)\n",
    "\n",
    "    # build a folder to save the results\n",
    "    sub_folder_name = folder_name + group + '/' + group_cat + '/'\n",
    "    if not os.path.exists(sub_folder_name):\n",
    "        os.makedirs(sub_folder_name)\n",
    "\n",
    "    feature_importance.to_csv(sub_folder_name + 'feature_importance.csv', index = False)\n",
    "    top_15_positive.to_csv(sub_folder_name + 'top_15_positive.csv', index = False)\n",
    "    top_15_negative.to_csv(sub_folder_name + 'top_15_negative.csv', index = False)\n",
    "\n",
    "    # save the mean of the metrics\n",
    "    metrics = pd.DataFrame({'accuracy': np.mean(accuracy_list), 'recall': np.mean(recall_list), 'precision': np.mean(precision_list), 'f1': np.mean(f1_list), 'roc_auc': np.mean(roc_auc_list)}, index = [0])\n",
    "    metrics.to_csv(sub_folder_name + 'metrics.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18623, 49)\n",
      "average accuracy:  0.8486815648901735\n",
      "average recall:  0.5851334293953852\n",
      "average precision:  0.7679692980600379\n",
      "average f1 score:  0.6641036069347583\n",
      "average roc auc score:  0.7621923530162531\n",
      "(8686, 49)\n",
      "average accuracy:  0.8411230425856253\n",
      "average recall:  0.551874282844073\n",
      "average precision:  0.729324176612425\n",
      "average f1 score:  0.62800348391284\n",
      "average roc auc score:  0.743009682663672\n",
      "(746, 49)\n",
      "average accuracy:  0.880724832214765\n",
      "average recall:  0.3011754385964912\n",
      "average precision:  0.5705050505050504\n",
      "average f1 score:  0.39262515262515263\n",
      "average roc auc score:  0.6344243241195704\n",
      "(9319, 49)\n",
      "average accuracy:  0.8412917925078155\n",
      "average recall:  0.6575916779559444\n",
      "average precision:  0.7749745275570239\n",
      "average f1 score:  0.7113819161035778\n",
      "average roc auc score:  0.7883793962221805\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# start from all-clear case:  further filter out the samples with missing value of the used features\n",
    "\n",
    "group = 'religon'\n",
    "\n",
    "for group_cat in data_religion_dict.keys():\n",
    "\n",
    "    data_group = data_religion_dict[group_cat]\n",
    "\n",
    "    X_categorical_transformed, X_continuous_transformed, Y_target, enc_categorical_feature_list = utils.feature_process(data_group, numerical_feature_list, categorical_feature_list, target_variable,value_label_dict)\n",
    "\n",
    "    X_continuous_categorical = np.concatenate((X_continuous_transformed, X_categorical_transformed), axis=1)\n",
    "\n",
    "    model = LogisticRegression(l1_ratio = 0.5, max_iter = 500, solver = 'saga', penalty = 'elasticnet')\n",
    "\n",
    "    accuracy_list, recall_list, precision_list, f1_list, roc_auc_list, importance_list = utils.cross_validation(X_continuous_categorical, Y_target, model, k = 5)\n",
    "\n",
    "    print('average accuracy: ', np.mean(accuracy_list))\n",
    "    print('average recall: ', np.mean(recall_list))\n",
    "    print('average precision: ', np.mean(precision_list))\n",
    "    print('average f1 score: ', np.mean(f1_list))\n",
    "    print('average roc auc score: ', np.mean(roc_auc_list))\n",
    "\n",
    "    # build the feature importance dataframe\n",
    "    feature_importance = pd.DataFrame({'feature': numerical_feature_list + enc_categorical_feature_list, 'importance': np.mean(importance_list, axis=0)})\n",
    "\n",
    "    top_15_positive = feature_importance.sort_values('importance', ascending = False).head(15)\n",
    "    top_15_negative = feature_importance.sort_values('importance', ascending = True).head(15)\n",
    "\n",
    "    # build a folder to save the results\n",
    "    sub_folder_name = folder_name + group + '/' + group_cat + '/'\n",
    "    if not os.path.exists(sub_folder_name):\n",
    "        os.makedirs(sub_folder_name)\n",
    "\n",
    "    feature_importance.to_csv(sub_folder_name + 'feature_importance.csv', index = False)\n",
    "    top_15_positive.to_csv(sub_folder_name + 'top_15_positive.csv', index = False)\n",
    "    top_15_negative.to_csv(sub_folder_name + 'top_15_negative.csv', index = False)\n",
    "\n",
    "    # save the mean of the metrics\n",
    "    metrics = pd.DataFrame({'accuracy': np.mean(accuracy_list), 'recall': np.mean(recall_list), 'precision': np.mean(precision_list), 'f1': np.mean(f1_list), 'roc_auc': np.mean(roc_auc_list)}, index = [0])\n",
    "    metrics.to_csv(sub_folder_name + 'metrics.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_1.13_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
