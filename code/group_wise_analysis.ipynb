{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance analysis acroos different group (race/religon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020.0\n",
      "1948.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "# import model\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# data path\n",
    "file_path = '../data/cumulative_2022_v3_9_domain.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "column_to_variable_dict = np.load('../data/column_to_variable_dict.npy', allow_pickle=True).item()\n",
    "variable_to_column_dict = np.load('../data/variable_to_column_dict.npy', allow_pickle=True).item()\n",
    "\n",
    "value_label_dict = np.load('../data/value_labels.npy', allow_pickle=True).item()\n",
    "\n",
    "# check the \"Year\" column's max and min value\n",
    "print(data['Year'].max())\n",
    "print(data['Year'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value = utils.missing_value_analysis(data)\n",
    "\n",
    "threshold_list = [0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "must_include_list = ['urbanism']\n",
    "\n",
    "used_features, not_used_features, folder_name = utils.feature_filter(data, threshold_list,column_to_variable_dict, must_include_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing value of the non-feature variable: \n",
      "Voted                0.091551\n",
      "Registered_voted     0.218061\n",
      "Voted_party          0.536483\n",
      "Vote_Nonvote_Pres    0.377067\n",
      "Race3                0.024874\n",
      "Race4                0.024874\n",
      "Race7                0.024068\n",
      "religion             0.007065\n",
      "Year                 0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# use the used features to filter out the data\n",
    "\n",
    "# set the target variable set and index variable set, these variables will not be used for training\n",
    "\n",
    "target_variable_list = ['Voted','Registered_voted','Voted_party','Vote_Nonvote_Pres']\n",
    "\n",
    "race_variable_list = ['Race3','Race4','Race7']\n",
    "\n",
    "religion_variable_list = ['religion']\n",
    "\n",
    "index_variable_list = ['Year', ]\n",
    "\n",
    "state_variable_list = ['State']\n",
    "\n",
    "non_feature_list = target_variable_list +  race_variable_list + religion_variable_list + index_variable_list\n",
    "\n",
    "# check the missing ratio of the target variable\n",
    "print('missing value of the non-feature variable: ')\n",
    "print(data[non_feature_list].isnull().sum() / len(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples who not vote :  17790\n",
      "number of samples who vote :  44188\n",
      "number of samples who vote case DK :  0\n",
      "(61978, 118)\n",
      "(37513, 118)\n",
      "number of numerical features:  9\n",
      "number of categorical features:  40\n",
      "numerical features list: ['therm_ChrFundament', 'therm_hispanics', 'therm_RepParty', 'therm_DemParty', 'therm_Whites', 'therm_liberals', 'therm_conservatives', 'therm_Blacks', 'Age']\n"
     ]
    }
   ],
   "source": [
    "target_variable = 'Voted'\n",
    "\n",
    "'''Voted  {0.0: '0. DK; NA; no Post IW; refused to say if voted;', 1.0: '1. No, did not vote', 2.0: '2. Yes, voted'}'''\n",
    "\n",
    "# filter out the samples with missing value of the target variable,drop the index\n",
    "data_new = data[data[target_variable].notnull()]\n",
    "# filter out the samples with target variable value = 0, count the number of samples whose target variable value = 0, 1 or 2\n",
    "print('number of samples who not vote : ', len(data_new[data_new[target_variable] == 1]))\n",
    "print('number of samples who vote : ', len(data_new[data_new[target_variable] == 2]))\n",
    "print('number of samples who vote case DK : ', len(data_new[data_new[target_variable] == 0]))\n",
    "\n",
    "data_new = data_new[data_new[target_variable] != 0]\n",
    "data_new = data_new.reset_index(drop=True)\n",
    "print(data_new.shape)\n",
    "\n",
    "year_threshold = 1982\n",
    "\n",
    "folder_name = folder_name + '/'+ str(year_threshold)+ '/'\n",
    "\n",
    "# filter out the samples whose year > year_threshold\n",
    "data_new = data_new[data_new['Year'] > year_threshold]\n",
    "data_new = data_new.reset_index(drop=True)\n",
    "print(data_new.shape)\n",
    "\n",
    "\n",
    "numerical_feature_list, categorical_feature_list = utils.feature_type_analysis(data, used_features, non_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples of White:  26879\n",
      "number of samples of Black:  4655\n",
      "number of samples of Asian:  696\n",
      "number of samples of American_Indian:  428\n",
      "number of samples of Hispanic:  3849\n",
      "number of samples of Other:  725\n",
      "number of samples of Protestant:  18623\n",
      "number of samples of Catholic:  8686\n",
      "number of samples of Jewish:  746\n",
      "number of samples of Other:  9319\n"
     ]
    }
   ],
   "source": [
    "# slipt the group by race and religion\n",
    "\n",
    "data_race7_dict = utils.group_split_race7(data_new) \n",
    "data_religion_dict = utils.group_split_religon(data_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26879, 50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average accuracy:  0.857844545957918\n",
      "average recall:  0.5902974140840849\n",
      "average precision:  0.7661200618794732\n",
      "average f1 score:  0.6667050852190337\n",
      "average roc auc score:  0.766497464955573\n",
      "(4655, 50)\n",
      "average accuracy:  0.8161117078410312\n",
      "average recall:  0.5658855535320377\n",
      "average precision:  0.7169315645735456\n",
      "average f1 score:  0.6322157806803577\n",
      "average roc auc score:  0.7395545146375999\n",
      "(696, 50)\n",
      "average accuracy:  0.794563206577595\n",
      "average recall:  0.5269227970084763\n",
      "average precision:  0.6782051282051281\n",
      "average f1 score:  0.5916610997462569\n",
      "average roc auc score:  0.7131760024691828\n",
      "(428, 50)\n",
      "average accuracy:  0.7569630642954857\n",
      "average recall:  0.6892585321655089\n",
      "average precision:  0.7031446351802303\n",
      "average f1 score:  0.6942463412974214\n",
      "average roc auc score:  0.7465784968500943\n",
      "(3849, 50)\n",
      "average accuracy:  0.7843598534105687\n",
      "average recall:  0.6491549672106925\n",
      "average precision:  0.7342200353076385\n",
      "average f1 score:  0.6881180998862091\n",
      "average roc auc score:  0.7564110402840791\n",
      "(725, 50)\n",
      "average accuracy:  0.8248275862068966\n",
      "average recall:  0.5311111111111111\n",
      "average precision:  0.6606016991504247\n",
      "average f1 score:  0.5866515286675857\n",
      "average roc auc score:  0.7231942393131064\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# start from all-clear case:  further filter out the samples with missing value of the used features\n",
    "\n",
    "group = 'race'\n",
    "\n",
    "for group_cat in data_race7_dict.keys():\n",
    "\n",
    "    data_group = data_race7_dict[group_cat]\n",
    "\n",
    "    X_categorical_transformed, X_continuous_transformed, Y_target, enc_categorical_feature_list = utils.feature_process(data_group, numerical_feature_list, categorical_feature_list, target_variable,value_label_dict)\n",
    "\n",
    "    X_continuous_categorical = np.concatenate((X_continuous_transformed, X_categorical_transformed), axis=1)\n",
    "\n",
    "    model = LogisticRegression(l1_ratio = 0.5, max_iter = 500, solver = 'saga', penalty = 'elasticnet')\n",
    "\n",
    "    accuracy_list, recall_list, precision_list, f1_list, roc_auc_list, importance_list = utils.cross_validation(X_continuous_categorical, Y_target, model, k = 5)\n",
    "\n",
    "    print('average accuracy: ', np.mean(accuracy_list))\n",
    "    print('average recall: ', np.mean(recall_list))\n",
    "    print('average precision: ', np.mean(precision_list))\n",
    "    print('average f1 score: ', np.mean(f1_list))\n",
    "    print('average roc auc score: ', np.mean(roc_auc_list))\n",
    "\n",
    "    # build the feature importance dataframe\n",
    "    feature_importance = pd.DataFrame({'feature': numerical_feature_list + enc_categorical_feature_list, 'importance': np.mean(importance_list, axis=0)})\n",
    "\n",
    "    top_15_positive = feature_importance.sort_values('importance', ascending = False).head(15)\n",
    "    top_15_negative = feature_importance.sort_values('importance', ascending = True).head(15)\n",
    "\n",
    "    # build a folder to save the results\n",
    "    sub_folder_name = folder_name + group + '/' + group_cat + '/'\n",
    "    if not os.path.exists(sub_folder_name):\n",
    "        os.makedirs(sub_folder_name)\n",
    "\n",
    "    feature_importance.to_csv(sub_folder_name + 'feature_importance.csv', index = False)\n",
    "    top_15_positive.to_csv(sub_folder_name + 'top_15_voter.csv', index = False)\n",
    "    top_15_negative.to_csv(sub_folder_name + 'top_15_non_voter.csv', index = False)\n",
    "\n",
    "    # save the mean of the metrics\n",
    "    metrics = pd.DataFrame({'accuracy': np.mean(accuracy_list), 'recall': np.mean(recall_list), 'precision': np.mean(precision_list), 'f1': np.mean(f1_list), 'roc_auc': np.mean(roc_auc_list)}, index = [0])\n",
    "    metrics.to_csv(sub_folder_name + 'metrics.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18623, 50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average accuracy:  0.8480371686647107\n",
      "average recall:  0.5845162305310698\n",
      "average precision:  0.7658612660156082\n",
      "average f1 score:  0.6629330300518849\n",
      "average roc auc score:  0.7615593240322965\n",
      "(8686, 50)\n",
      "average accuracy:  0.8411231750839543\n",
      "average recall:  0.5523585929784071\n",
      "average precision:  0.7290241880938019\n",
      "average f1 score:  0.6281950296782736\n",
      "average roc auc score:  0.7431751796911741\n",
      "(746, 50)\n",
      "average accuracy:  0.8780492170022371\n",
      "average recall:  0.3091754385964912\n",
      "average precision:  0.5468686868686868\n",
      "average f1 score:  0.39288689611270255\n",
      "average roc auc score:  0.6361456378935157\n",
      "(9319, 50)\n",
      "average accuracy:  0.8415062695960873\n",
      "average recall:  0.6603457886651961\n",
      "average precision:  0.7736979566419017\n",
      "average f1 score:  0.7124332614824447\n",
      "average roc auc score:  0.7893140910932284\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# start from all-clear case:  further filter out the samples with missing value of the used features\n",
    "\n",
    "group = 'religon'\n",
    "\n",
    "for group_cat in data_religion_dict.keys():\n",
    "\n",
    "    data_group = data_religion_dict[group_cat]\n",
    "\n",
    "    X_categorical_transformed, X_continuous_transformed, Y_target, enc_categorical_feature_list = utils.feature_process(data_group, numerical_feature_list, categorical_feature_list, target_variable,value_label_dict)\n",
    "\n",
    "    X_continuous_categorical = np.concatenate((X_continuous_transformed, X_categorical_transformed), axis=1)\n",
    "\n",
    "    model = LogisticRegression(l1_ratio = 0.5, max_iter = 500, solver = 'saga', penalty = 'elasticnet')\n",
    "\n",
    "    accuracy_list, recall_list, precision_list, f1_list, roc_auc_list, importance_list = utils.cross_validation(X_continuous_categorical, Y_target, model, k = 5)\n",
    "\n",
    "    print('average accuracy: ', np.mean(accuracy_list))\n",
    "    print('average recall: ', np.mean(recall_list))\n",
    "    print('average precision: ', np.mean(precision_list))\n",
    "    print('average f1 score: ', np.mean(f1_list))\n",
    "    print('average roc auc score: ', np.mean(roc_auc_list))\n",
    "\n",
    "    # build the feature importance dataframe\n",
    "    feature_importance = pd.DataFrame({'feature': numerical_feature_list + enc_categorical_feature_list, 'importance': np.mean(importance_list, axis=0)})\n",
    "\n",
    "    top_15_positive = feature_importance.sort_values('importance', ascending = False).head(15)\n",
    "    top_15_negative = feature_importance.sort_values('importance', ascending = True).head(15)\n",
    "\n",
    "    # build a folder to save the results\n",
    "    sub_folder_name = folder_name + group + '/' + group_cat + '/'\n",
    "    if not os.path.exists(sub_folder_name):\n",
    "        os.makedirs(sub_folder_name)\n",
    "\n",
    "    feature_importance.to_csv(sub_folder_name + 'feature_importance.csv', index = False)\n",
    "    top_15_positive.to_csv(sub_folder_name + 'top_15_voter.csv', index = False)\n",
    "    top_15_negative.to_csv(sub_folder_name + 'top_15_non_voter.csv', index = False)\n",
    "\n",
    "    # save the mean of the metrics\n",
    "    metrics = pd.DataFrame({'accuracy': np.mean(accuracy_list), 'recall': np.mean(recall_list), 'precision': np.mean(precision_list), 'f1': np.mean(f1_list), 'roc_auc': np.mean(roc_auc_list)}, index = [0])\n",
    "    metrics.to_csv(sub_folder_name + 'metrics.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_1.13_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
