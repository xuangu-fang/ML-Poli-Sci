{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance analysis acroos different group (race/religon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020.0\n",
      "1948.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "\n",
    "# data path\n",
    "file_path = '../data/cumulative_2022_v3_9_domain.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "column_to_variable_dict = np.load('../data/column_to_variable_dict.npy', allow_pickle=True).item()\n",
    "variable_to_column_dict = np.load('../data/variable_to_column_dict.npy', allow_pickle=True).item()\n",
    "\n",
    "# check the \"Year\" column's max and min value\n",
    "print(data['Year'].max())\n",
    "print(data['Year'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value = utils.missing_value_analysis(data)\n",
    "\n",
    "threshold_list = [0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "used_features, not_used_features, folder_name = utils.feature_filter(data, threshold_list,column_to_variable_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one column to indicate the variable name of each row,using the index of the missing_value as the key\n",
    "\n",
    "variable_name = [ column_to_variable_dict[var] for var in missing_value.index]\n",
    "missing_value['variable_name'] = variable_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing value of the target variable: \n",
      "Voted                0.091551\n",
      "Registered_voted     0.218061\n",
      "Voted_party          0.536483\n",
      "Vote_Nonvote_Pres    0.377067\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# use the used features to filter out the data\n",
    "\n",
    "# set the target variable set and index variable set, these variables will not be used for training\n",
    "\n",
    "target_variable_list = ['Voted','Registered_voted','Voted_party','Vote_Nonvote_Pres']\n",
    "\n",
    "index_variable_list = ['Year', ]\n",
    "\n",
    "# check the missing ratio of the target variable\n",
    "print('missing value of the target variable: ')\n",
    "print(data[target_variable_list].isnull().sum() / len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples who not vote :  17790\n",
      "number of samples who vote :  44188\n",
      "number of samples who vote case DK :  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(61978, 118)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_variable = 'Voted'\n",
    "\n",
    "'''Voted  {0.0: '0. DK; NA; no Post IW; refused to say if voted;', 1.0: '1. No, did not vote', 2.0: '2. Yes, voted'}'''\n",
    "\n",
    "# filter out the samples with missing value of the target variable,drop the index\n",
    "data_new = data[data[target_variable].notnull()]\n",
    "# filter out the samples with target variable value = 0, count the number of samples whose target variable value = 0, 1 or 2\n",
    "print('number of samples who not vote : ', len(data_new[data_new[target_variable] == 1]))\n",
    "print('number of samples who vote : ', len(data_new[data_new[target_variable] == 2]))\n",
    "print('number of samples who vote case DK : ', len(data_new[data_new[target_variable] == 0]))\n",
    "\n",
    "data_new = data_new[data_new[target_variable] != 0]\n",
    "data_new = data_new.reset_index(drop=True)\n",
    "data_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    44188\n",
       "1.0    17790\n",
       "Name: Voted, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new['Voted'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of numerical features:  9\n",
      "number of categorical features:  43\n",
      "numerical features list: ['therm_ChrFundament', 'therm_hispanics', 'therm_RepParty', 'therm_DemParty', 'therm_Whites', 'therm_liberals', 'therm_conservatives', 'therm_Blacks', 'Age']\n"
     ]
    }
   ],
   "source": [
    "# go through all used features, check the num of the categories of each feature: if the num of categories > 10, then this feature is a continuous/numerical feature, otherwise, this feature is a categorical feature-> need to do one-hot encoding\n",
    "\n",
    "numerical_feature_list = []\n",
    "categorical_feature_list = []\n",
    "\n",
    "for feature in used_features:\n",
    "\n",
    "    if feature not in target_variable_list and feature not in index_variable_list:\n",
    "\n",
    "        if len(data_new[feature].value_counts()) > 10:\n",
    "            numerical_feature_list.append(feature)\n",
    "        else:\n",
    "            categorical_feature_list.append(feature)\n",
    "\n",
    "print('number of numerical features: ', len(numerical_feature_list))\n",
    "\n",
    "print('number of categorical features: ', len(categorical_feature_list))\n",
    "\n",
    "print('numerical features list:',numerical_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61978, 70)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0    44188\n",
       "1.0    17790\n",
       "Name: Voted, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# start from all-clear case:  further filter out the samples with missing value of the used features\n",
    "\n",
    "data_XY = data_new[numerical_feature_list + categorical_feature_list+[target_variable]]\n",
    "# data_XY = data_XY[data_XY.notnull().all(axis=1)]\n",
    "# data_XY = data_XY.reset_index(drop=True)\n",
    "print(data_XY.shape)\n",
    "\n",
    "X_continuous = data_XY[numerical_feature_list]\n",
    "X_categorical = data_XY[categorical_feature_list]\n",
    "Y_target = data_XY[target_variable]\n",
    "\n",
    "Y_target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7150693772184575\n",
      "recall:  0.03727409638554217\n",
      "precision:  0.518324607329843\n",
      "f1 score:  0.06954689146469968\n",
      "number of predicted samples for each class: \n",
      "2.0    18212\n",
      "1.0      382\n",
      "dtype: int64\n",
      "model just predict the majority class:  0.020544261589760138\n"
     ]
    }
   ],
   "source": [
    "# only use the continuous features to do logistic regression by sklearn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# impute the missing value of the continuous features by using the mean value of the feature\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "X_continuous_imp = imp.fit_transform(X_continuous)\n",
    "\n",
    "X_continuous_transformed = StandardScaler().fit_transform(X_continuous_imp)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_continuous_transformed, Y_target, test_size=0.3, random_state=1)\n",
    "\n",
    "# use the default parameters\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "\n",
    "# get the accuracy, recall, precision, f1 score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "y_pred = logisticRegr.predict(X_test)\n",
    "\n",
    "print('accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('recall: ', recall_score(y_test, y_pred))\n",
    "print('precision: ', precision_score(y_test, y_pred))\n",
    "print('f1 score: ', f1_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "# count the number of predicted samples for each class\n",
    "print('number of predicted samples for each class: ')\n",
    "print(pd.Series(y_pred).value_counts())\n",
    "\n",
    "print('model just predict the majority class: ', pd.Series(y_pred).value_counts()[1] / len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8758739378294074\n",
      "recall:  0.7312957059816239\n",
      "precision:  0.8167539267015707\n",
      "f1 score:  0.7716660071230709\n",
      "number of predicted samples for each class: \n",
      "2.0    13819\n",
      "1.0     4775\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wukong/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "# only use the categorical features to do logistic regression by sklearn, for the NaN value, set as a new category, then do one-hot encoding\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X_categorical_imp = X_categorical.fillna(-1)\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(X_categorical_imp)\n",
    "X_categorical_transformed = enc.transform(X_categorical_imp).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_categorical_transformed, Y_target, test_size=0.3, random_state=0)\n",
    "\n",
    "# use the default parameters\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "\n",
    "# get the accuracy, recall, precision, f1 score\n",
    "\n",
    "y_pred = logisticRegr.predict(X_test)\n",
    "\n",
    "print('accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('recall: ', recall_score(y_test, y_pred))\n",
    "print('precision: ', precision_score(y_test, y_pred))\n",
    "print('f1 score: ', f1_score(y_test, y_pred))\n",
    "\n",
    "# count the number of predicted samples for each class\n",
    "print('number of predicted samples for each class: ')\n",
    "print(pd.Series(y_pred).value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 features: \n",
      "('Feature_id 4', 'satisfactionDemocracy', 'Category 1', -1.0)\n",
      "('Feature_id 25', 'VCF9028', 'Category 4', 3.0)\n",
      "('Feature_id 44', 'Interest_elections', 'Category 4', 3.0)\n",
      "('Feature_id 28', 'Pre_election_inten_vote', 'Category 4', 3.0)\n",
      "('Feature_id 21', 'church_attendance', 'Category 1', -1.0)\n",
      "('Feature_id 18', 'volunteer', 'Category 1', -1.0)\n",
      "('Feature_id 28', 'Pre_election_inten_vote', 'Category 5', 4.0)\n",
      "('Feature_id 12', 'VCF9022', 'Category 3', 5.0)\n",
      "('Feature_id 12', 'VCF9022', 'Category 2', 1.0)\n",
      "('Feature_id 12', 'VCF9022', 'Category 1', -1.0)\n"
     ]
    }
   ],
   "source": [
    "# 建立映射\n",
    "feature_mapping = {}\n",
    "current_index = 0\n",
    "for feature_index, categories in enumerate(enc.categories_):\n",
    "    for category_index, category in enumerate(categories):\n",
    "        feature_mapping[current_index] = (f'Feature_id {feature_index}', \n",
    "         enc.feature_names_in_[feature_index],                             \n",
    "        f'Category {category_index + 1}', category)\n",
    "        current_index += 1\n",
    "\n",
    "# # 打印映射结果\n",
    "# print(\"Feature Mapping:\")\n",
    "# for k, v in feature_mapping.items():\n",
    "#     print(f\"Encoded feature {k}: Original {v}\")\n",
    "\n",
    "# identify the top 10 features that have the largest absolute value of the coefficient,\n",
    "\n",
    "top_10_index = np.argsort(np.abs(logisticRegr.coef_[0]))[-10:]\n",
    "\n",
    "# print the top 10 features\n",
    "print('top 10 features: ')\n",
    "for index in top_10_index:\n",
    "    print(feature_mapping[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8796923738840486\n",
      "recall:  0.7416088505531596\n",
      "precision:  0.8215621105110096\n",
      "f1 score:  0.7795407509608752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wukong/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/wukong/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# concatenate the continuous features and categorical features, then do logistic regression\n",
    "\n",
    "X_continuous_categorical = np.concatenate((X_continuous_transformed, X_categorical_transformed), axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_continuous_categorical, Y_target, test_size=0.3, random_state=0)\n",
    "\n",
    "# use the default parameters\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "\n",
    "# get the accuracy, recall, precision, f1 score\n",
    "\n",
    "y_pred = logisticRegr.predict(X_test)\n",
    "\n",
    "print('accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('recall: ', recall_score(y_test, y_pred))\n",
    "print('precision: ', precision_score(y_test, y_pred))\n",
    "print('f1 score: ', f1_score(y_test, y_pred))\n",
    "\n",
    "# check the top 10 features with the highest absolute value of the coefficient\n",
    "\n",
    "feature_importance = pd.DataFrame({'feature': numerical_feature_list + enc.get_feature_names().tolist(), 'importance': logisticRegr.coef_[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8295148972786921\n",
      "recall:  0.7022313894618414\n",
      "precision:  0.7030223390275953\n",
      "f1 score:  0.7026266416510318\n"
     ]
    }
   ],
   "source": [
    "# try the decision tree model\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_continuous_categorical, Y_target, test_size=0.3, random_state=0)\n",
    "\n",
    "# use the default parameters\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# get the accuracy, recall, precision, f1 score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('recall: ', recall_score(y_test, y_pred))\n",
    "print('precision: ', precision_score(y_test, y_pred))\n",
    "print('f1 score: ', f1_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_1.13_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
