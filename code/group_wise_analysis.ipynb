{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "count the missing values of each dommain features, set the threshold to 0.3 to filter out the features with too many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020.0\n",
      "1948.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# data path\n",
    "file_path = '../data/cumulative_2022_v3_9_domain.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "column_to_variable_dict = np.load('../data/column_to_variable_dict.npy', allow_pickle=True).item()\n",
    "variable_to_column_dict = np.load('../data/variable_to_column_dict.npy', allow_pickle=True).item()\n",
    "\n",
    "# check the \"Year\" column's max and min value\n",
    "print(data['Year'].max())\n",
    "print(data['Year'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year                                  0\n",
       "South                              1801\n",
       "region                             1801\n",
       "racial_composition_nbhood         59420\n",
       "racial_composition_gradeSchool    60327\n",
       "                                  ...  \n",
       "occupation                        28223\n",
       "occupation14                      51795\n",
       "occupation71                      51795\n",
       "home_ownership                    13672\n",
       "urbanism                          24972\n",
       "Length: 118, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis the missing value of each column in differernt period:\n",
    "# 1. overall missing value and percentage\n",
    "# 2. missing value and percentage in recent 10 years (\"Year\" >= 2012)\")\n",
    "# 3. missing value and percentage in recent 20 years (\"Year\" >= 2002)\")\n",
    "# 4. missing value and percentage in recent 30 years (\"Year\" >= 1992)\")\n",
    "# 5. missing value and percentage in recent 40 years (\"Year\" >= 1982)\")\n",
    "# 6. missing value and percentage in recent 50 years (\"Year\" >= 1972)\")\n",
    "# 7. missing value and percentage in recent 60 years (\"Year\" >= 1962)\")\n",
    "\n",
    "# save the result in csv file, the first column is the feature name\n",
    "\n",
    "\n",
    "def missing_value_analysis(data):\n",
    "    # get the number of missing value of each column\n",
    "    missing_value_num = data.isnull().sum()\n",
    "    # get the percentage of missing value of each column\n",
    "    missing_value_percentage = missing_value_num / len(data)\n",
    "\n",
    "    missing_value_percentage_10 = data[data['Year'] >= 2012].isnull().sum() / len(data[data['Year'] >= 2012])\n",
    "    missing_value_percentage_20 = data[data['Year'] >= 2002].isnull().sum() / len(data[data['Year'] >= 2002])\n",
    "    missing_value_percentage_30 = data[data['Year'] >= 1992].isnull().sum() / len(data[data['Year'] >= 1992])\n",
    "    missing_value_percentage_40 = data[data['Year'] >= 1982].isnull().sum() / len(data[data['Year'] >= 1982])\n",
    "    missing_value_percentage_50 = data[data['Year'] >= 1972].isnull().sum() / len(data[data['Year'] >= 1972])\n",
    "    missing_value_percentage_60 = data[data['Year'] >= 1962].isnull().sum() / len(data[data['Year'] >= 1962])\n",
    "    missing_value_percentage_70 = data[data['Year'] >= 1952].isnull().sum() / len(data[data['Year'] >= 1952])\n",
    "\n",
    "    # get the variable name of each column by using the column_to_variable_dict\n",
    "    # missing_value_num.index = column_to_variable_dict['variable']\n",
    "\n",
    "\n",
    "    # combine the result\n",
    "    missing_value = pd.concat([missing_value_num, missing_value_percentage,\n",
    "                               missing_value_percentage_10, missing_value_percentage_20,\n",
    "                               missing_value_percentage_30, missing_value_percentage_40,\n",
    "                               missing_value_percentage_50, missing_value_percentage_60,    missing_value_percentage_70], axis=1)\n",
    "    missing_value.columns = ['missing_value_num', 'missing_value_percentage',\n",
    "                                'missing_value_percentage_10(>=2012)', 'missing_value_percentage_20(>=2002)',\n",
    "                                'missing_value_percentage_30(>=1992)', 'missing_value_percentage_40(>=1982)',\n",
    "                                'missing_value_percentage_50(>=1972)', 'missing_value_percentage_60(>=1962)', 'missing_value_percentage_60(>=1952)']\n",
    "\n",
    "    # sort the result by missing value percentage\n",
    "    missing_value = missing_value.sort_values(by='missing_value_percentage', ascending=False)\n",
    "\n",
    "\n",
    "    return missing_value\n",
    "\n",
    "missing_value = missing_value_analysis(data)\n",
    "\n",
    "# save the result\n",
    "# massing_value.to_csv('../data/missing_value_analysis.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one column to indicate the variable name of each row,using the index of the missing_value as the key\n",
    "\n",
    "variable_name = [ column_to_variable_dict[var] for var in missing_value.index]\n",
    "missing_value['variable_name'] = variable_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features used:  74\n",
      "number of features not used:  44\n"
     ]
    }
   ],
   "source": [
    "# set the filter-out thresholds:\n",
    "# 1. missing_value_percentage_10(>=2012) < 0.3\n",
    "# 2. missing_value_percentage_20(>=2002) < 0.4\n",
    "# 3. missing_value_percentage_30(>=1992) < 0.5\n",
    "\n",
    "threshold_10 = 0.3\n",
    "threshold_20 = 0.4\n",
    "threshold_30 = 0.5\n",
    "\n",
    "\n",
    "# filter out the features\n",
    "missing_value_used = missing_value[(\n",
    "                missing_value['missing_value_percentage_10(>=2012)'] < threshold_10) & \n",
    "                                        (missing_value['missing_value_percentage_20(>=2002)'] < threshold_20) &\n",
    "                                        (missing_value['missing_value_percentage_30(>=1992)'] < threshold_30)]\n",
    "\n",
    "missing_value_not_used = missing_value[(\n",
    "                missing_value['missing_value_percentage_10(>=2012)'] >= threshold_10) | \n",
    "                                        (missing_value['missing_value_percentage_20(>=2002)'] >= threshold_20) |\n",
    "                                        (missing_value['missing_value_percentage_30(>=1992)'] >= threshold_30)]\n",
    "\n",
    "# count the number of features\n",
    "print('number of features used: ', len(missing_value_used))\n",
    "print('number of features not used: ', len(missing_value_not_used))\n",
    "\n",
    "\n",
    "\n",
    "# save the result\n",
    "# make folder namsed with threshold:\n",
    "\n",
    "folder_name = '../data/threshold_10_' + str(threshold_10) + '_threshold_20_' + str(threshold_20) + '_threshold_30_' + str(threshold_30)\n",
    "\n",
    "# make folder if not exist\n",
    "import os\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# missing_value_used.to_csv(folder_name + '/missing_value_analysis_used.csv')\n",
    "# missing_value_not_used.to_csv(folder_name + '/missing_value_analysis_not_used.csv')\n",
    "\n",
    "# save the used features names (row names) and the variable names\n",
    "used_features = missing_value_used.index.tolist()\n",
    "with open(folder_name + '/used_features.txt', 'w') as f:\n",
    "    for item in used_features:\n",
    "        f.write(\"%s (%s)\\n\" % (item, column_to_variable_dict[item]))\n",
    "\n",
    "# save the not used features names (row names)\n",
    "not_used_features = missing_value_not_used.index.tolist()\n",
    "with open(folder_name + '/not_used_features.txt', 'w') as f:\n",
    "    for item in not_used_features:\n",
    "        f.write(\"%s (%s)\\n\" % (item, column_to_variable_dict[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing value of the target variable: \n",
      "Voted                0.091551\n",
      "Registered_voted     0.218061\n",
      "Voted_party          0.536483\n",
      "Vote_Nonvote_Pres    0.377067\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# use the used features to filter out the data\n",
    "\n",
    "# set the target variable set and index variable set, these variables will not be used for training\n",
    "\n",
    "target_variable_list = ['Voted','Registered_voted','Voted_party','Vote_Nonvote_Pres']\n",
    "\n",
    "index_variable_list = ['Year', ]\n",
    "\n",
    "# check the missing ratio of the target variable\n",
    "print('missing value of the target variable: ')\n",
    "print(data[target_variable_list].isnull().sum() / len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples who not vote :  17790\n",
      "number of samples who vote :  44188\n",
      "number of samples who vote case DK :  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(61978, 118)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_variable = 'Voted'\n",
    "\n",
    "'''Voted  {0.0: '0. DK; NA; no Post IW; refused to say if voted;', 1.0: '1. No, did not vote', 2.0: '2. Yes, voted'}'''\n",
    "\n",
    "# filter out the samples with missing value of the target variable,drop the index\n",
    "data_new = data[data[target_variable].notnull()]\n",
    "# filter out the samples with target variable value = 0, count the number of samples whose target variable value = 0, 1 or 2\n",
    "print('number of samples who not vote : ', len(data_new[data_new[target_variable] == 1]))\n",
    "print('number of samples who vote : ', len(data_new[data_new[target_variable] == 2]))\n",
    "print('number of samples who vote case DK : ', len(data_new[data_new[target_variable] == 0]))\n",
    "\n",
    "data_new = data_new[data_new[target_variable] != 0]\n",
    "data_new = data_new.reset_index(drop=True)\n",
    "data_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    44188\n",
       "1.0    17790\n",
       "Name: Voted, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new['Voted'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of numerical features:  12\n",
      "number of categorical features:  57\n",
      "numerical features list: ['therm_Christians', 'therm_Mislims', 'religion_fullCode', 'therm_ChrFundament', 'therm_hispanics', 'therm_RepParty', 'therm_DemParty', 'therm_Whites', 'therm_liberals', 'therm_conservatives', 'therm_Blacks', 'Age']\n"
     ]
    }
   ],
   "source": [
    "# go through all used features, check the num of the categories of each feature: if the num of categories > 10, then this feature is a continuous/numerical feature, otherwise, this feature is a categorical feature-> need to do one-hot encoding\n",
    "\n",
    "numerical_feature_list = []\n",
    "categorical_feature_list = []\n",
    "\n",
    "for feature in used_features:\n",
    "\n",
    "    if feature not in target_variable_list and feature not in index_variable_list:\n",
    "\n",
    "        if len(data_new[feature].value_counts()) > 10:\n",
    "            numerical_feature_list.append(feature)\n",
    "        else:\n",
    "            categorical_feature_list.append(feature)\n",
    "\n",
    "print('number of numerical features: ', len(numerical_feature_list))\n",
    "\n",
    "print('number of categorical features: ', len(categorical_feature_list))\n",
    "\n",
    "print('numerical features list:',numerical_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61978, 70)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0    44188\n",
       "1.0    17790\n",
       "Name: Voted, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# start from all-clear case:  further filter out the samples with missing value of the used features\n",
    "\n",
    "data_XY = data_new[numerical_feature_list + categorical_feature_list+[target_variable]]\n",
    "# data_XY = data_XY[data_XY.notnull().all(axis=1)]\n",
    "# data_XY = data_XY.reset_index(drop=True)\n",
    "print(data_XY.shape)\n",
    "\n",
    "X_continuous = data_XY[numerical_feature_list]\n",
    "X_categorical = data_XY[categorical_feature_list]\n",
    "Y_target = data_XY[target_variable]\n",
    "\n",
    "Y_target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7150693772184575\n",
      "recall:  0.03727409638554217\n",
      "precision:  0.518324607329843\n",
      "f1 score:  0.06954689146469968\n",
      "number of predicted samples for each class: \n",
      "2.0    18212\n",
      "1.0      382\n",
      "dtype: int64\n",
      "model just predict the majority class:  0.020544261589760138\n"
     ]
    }
   ],
   "source": [
    "# only use the continuous features to do logistic regression by sklearn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# impute the missing value of the continuous features by using the mean value of the feature\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "X_continuous_imp = imp.fit_transform(X_continuous)\n",
    "\n",
    "X_continuous_transformed = StandardScaler().fit_transform(X_continuous_imp)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_continuous_transformed, Y_target, test_size=0.3, random_state=1)\n",
    "\n",
    "# use the default parameters\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "\n",
    "# get the accuracy, recall, precision, f1 score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "y_pred = logisticRegr.predict(X_test)\n",
    "\n",
    "print('accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('recall: ', recall_score(y_test, y_pred))\n",
    "print('precision: ', precision_score(y_test, y_pred))\n",
    "print('f1 score: ', f1_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "# count the number of predicted samples for each class\n",
    "print('number of predicted samples for each class: ')\n",
    "print(pd.Series(y_pred).value_counts())\n",
    "\n",
    "print('model just predict the majority class: ', pd.Series(y_pred).value_counts()[1] / len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8758739378294074\n",
      "recall:  0.7312957059816239\n",
      "precision:  0.8167539267015707\n",
      "f1 score:  0.7716660071230709\n",
      "number of predicted samples for each class: \n",
      "2.0    13819\n",
      "1.0     4775\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wukong/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "# only use the categorical features to do logistic regression by sklearn, for the NaN value, set as a new category, then do one-hot encoding\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X_categorical_imp = X_categorical.fillna(-1)\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(X_categorical_imp)\n",
    "X_categorical_transformed = enc.transform(X_categorical_imp).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_categorical_transformed, Y_target, test_size=0.3, random_state=0)\n",
    "\n",
    "# use the default parameters\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "\n",
    "# get the accuracy, recall, precision, f1 score\n",
    "\n",
    "y_pred = logisticRegr.predict(X_test)\n",
    "\n",
    "print('accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('recall: ', recall_score(y_test, y_pred))\n",
    "print('precision: ', precision_score(y_test, y_pred))\n",
    "print('f1 score: ', f1_score(y_test, y_pred))\n",
    "\n",
    "# count the number of predicted samples for each class\n",
    "print('number of predicted samples for each class: ')\n",
    "print(pd.Series(y_pred).value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 features: \n",
      "('Feature_id 4', 'satisfactionDemocracy', 'Category 1', -1.0)\n",
      "('Feature_id 25', 'VCF9028', 'Category 4', 3.0)\n",
      "('Feature_id 44', 'Interest_elections', 'Category 4', 3.0)\n",
      "('Feature_id 28', 'Pre_election_inten_vote', 'Category 4', 3.0)\n",
      "('Feature_id 21', 'church_attendance', 'Category 1', -1.0)\n",
      "('Feature_id 18', 'volunteer', 'Category 1', -1.0)\n",
      "('Feature_id 28', 'Pre_election_inten_vote', 'Category 5', 4.0)\n",
      "('Feature_id 12', 'VCF9022', 'Category 3', 5.0)\n",
      "('Feature_id 12', 'VCF9022', 'Category 2', 1.0)\n",
      "('Feature_id 12', 'VCF9022', 'Category 1', -1.0)\n"
     ]
    }
   ],
   "source": [
    "# 建立映射\n",
    "feature_mapping = {}\n",
    "current_index = 0\n",
    "for feature_index, categories in enumerate(enc.categories_):\n",
    "    for category_index, category in enumerate(categories):\n",
    "        feature_mapping[current_index] = (f'Feature_id {feature_index}', \n",
    "         enc.feature_names_in_[feature_index],                             \n",
    "        f'Category {category_index + 1}', category)\n",
    "        current_index += 1\n",
    "\n",
    "# # 打印映射结果\n",
    "# print(\"Feature Mapping:\")\n",
    "# for k, v in feature_mapping.items():\n",
    "#     print(f\"Encoded feature {k}: Original {v}\")\n",
    "\n",
    "# identify the top 10 features that have the largest absolute value of the coefficient,\n",
    "\n",
    "top_10_index = np.argsort(np.abs(logisticRegr.coef_[0]))[-10:]\n",
    "\n",
    "# print the top 10 features\n",
    "print('top 10 features: ')\n",
    "for index in top_10_index:\n",
    "    print(feature_mapping[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8796923738840486\n",
      "recall:  0.7416088505531596\n",
      "precision:  0.8215621105110096\n",
      "f1 score:  0.7795407509608752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wukong/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/wukong/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# concatenate the continuous features and categorical features, then do logistic regression\n",
    "\n",
    "X_continuous_categorical = np.concatenate((X_continuous_transformed, X_categorical_transformed), axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_continuous_categorical, Y_target, test_size=0.3, random_state=0)\n",
    "\n",
    "# use the default parameters\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "\n",
    "# get the accuracy, recall, precision, f1 score\n",
    "\n",
    "y_pred = logisticRegr.predict(X_test)\n",
    "\n",
    "print('accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('recall: ', recall_score(y_test, y_pred))\n",
    "print('precision: ', precision_score(y_test, y_pred))\n",
    "print('f1 score: ', f1_score(y_test, y_pred))\n",
    "\n",
    "# check the top 10 features with the highest absolute value of the coefficient\n",
    "\n",
    "feature_importance = pd.DataFrame({'feature': numerical_feature_list + enc.get_feature_names().tolist(), 'importance': logisticRegr.coef_[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8295148972786921\n",
      "recall:  0.7022313894618414\n",
      "precision:  0.7030223390275953\n",
      "f1 score:  0.7026266416510318\n"
     ]
    }
   ],
   "source": [
    "# try the decision tree model\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_continuous_categorical, Y_target, test_size=0.3, random_state=0)\n",
    "\n",
    "# use the default parameters\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# get the accuracy, recall, precision, f1 score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('recall: ', recall_score(y_test, y_pred))\n",
    "print('precision: ', precision_score(y_test, y_pred))\n",
    "print('f1 score: ', f1_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_1.13_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
